---
title: 기능 선택 (데이터 마이닝) | Microsoft Docs
ms.custom: ''
ms.date: 03/06/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- mining models [Analysis Services], feature selections
- attributes [data mining]
- feature selection algorithms [Analysis Services]
- data mining [Analysis Services], feature selections
- neural network algorithms [Analysis Services]
- naive bayes algorithms [Analysis Services]
- decision tree algorithms [Analysis Services]
- datasets [Analysis Services]
- clustering algorithms [Analysis Services]
- coding [Data Mining]
ms.assetid: b044e785-4875-45ab-8ae4-cd3b4e3033bb
author: minewiskan
ms.author: owend
ms.openlocfilehash: ef5ee56636e7710074a893aed733905e1bf983bd
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 08/04/2020
ms.locfileid: "87652450"
---
# <a name="feature-selection-data-mining"></a><span data-ttu-id="7c29c-102">기능 선택(데이터 마이닝)</span><span class="sxs-lookup"><span data-stu-id="7c29c-102">Feature Selection (Data Mining)</span></span>
  <span data-ttu-id="7c29c-103">*기능 선택* 은 처리 및 분석을 위해 입력을 관리 하기 쉬운 크기로 줄이는 데 사용할 수 있는 도구와 기법을 설명 하기 위해 데이터 마이닝에 일반적으로 사용 되는 용어입니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-103">*Feature selection* is a term commonly used in data mining to describe the tools and techniques available for reducing inputs to a manageable size for processing and analysis.</span></span> <span data-ttu-id="7c29c-104">기능 선택은 모델을 작성할 때 고려할 수 있는 특성의 수에 대해 임의 또는 미리 정의 된 *구분을 설정*하는 것을 의미 하며,이는 분석가 또는 모델링 도구가 분석에 대 한 유용성을 기준으로 적극적으로 특성을 선택 하거나 삭제 하는 특성을 선택 하는 것을 의미 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-104">Feature selection implies not only *cardinality reduction*, which means imposing an arbitrary or predefined cutoff on the number of attributes that can be considered when building a model, but also the choice of attributes, meaning that either the analyst or the modeling tool actively selects or discards attributes based on their usefulness for analysis.</span></span>  
  
 <span data-ttu-id="7c29c-105">데이터 세트에는 모델을 작성하는 데 필요한 것보다 훨씬 많은 정보가 자주 포함되기 때문에 기능 선택을 적용하는 기능은 효과적인 분석에 중요합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-105">The ability to apply feature selection is critical for effective analysis, because datasets frequently contain far more information than is needed to build the model.</span></span> <span data-ttu-id="7c29c-106">예를 들어, 데이터 세트에 고객의 특성을 설명하는 열이 500개 있지만 일부 열에 데이터가 매우 드물 경우 해당 열을 모델에 추가하여 얻는 이점이 거의 없을 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-106">For example, a dataset might contain 500 columns that describe the characteristics of customers, but if the data in some of the columns is very sparse you would gain very little benefit from adding them to the model.</span></span> <span data-ttu-id="7c29c-107">모델을 작성하는 동안 불필요한 열을 그대로 둘 경우 학습 프로세스에 더 많은 CPU와 메모리가 필요하고 완료된 모델에 더 많은 스토리지 공간이 필요합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-107">If you keep the unneeded columns while building the model, more CPU and memory are required during the training process, and more storage space is required for the completed model.</span></span>  
  
 <span data-ttu-id="7c29c-108">리소스가 문제 되지 않는 경우라도 불필요한 열은 다음과 같은 이유로 검색된 패턴의 품질을 떨어뜨리므로 일반적으로 제거하는 것이 좋습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-108">Even if resources are not an issue, you typically want to remove unneeded columns because they might degrade the quality of discovered patterns, for the following reasons:</span></span>  
  
-   <span data-ttu-id="7c29c-109">일부 열에 잡음이 있거나 중복됩니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-109">Some columns are noisy or redundant.</span></span> <span data-ttu-id="7c29c-110">이러한 잡음이 있으면 데이터에서 의미 있는 패턴을 찾아내기가 더 어려워집니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-110">This noise makes it more difficult to discover meaningful patterns from the data;</span></span>  
  
-   <span data-ttu-id="7c29c-111">양질의 패턴을 찾아내기 위해 대부분의 데이터 마이닝 알고리즘은 고차원 데이터 집합에 대해 훨씬 큰 학습 데이터 집합을 요구합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-111">To discover quality patterns, most data mining algorithms require much larger training data set on high-dimensional data set.</span></span> <span data-ttu-id="7c29c-112">하지만 일부 데이터 마이닝 애플리케이션에서는 학습 데이터가 매우 작습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-112">But the training data is very small in some data mining applications.</span></span>  
  
 <span data-ttu-id="7c29c-113">데이터 원본의 500개 열 중에서 50개 열에만 모델 작성에 유용한 정보가 있을 경우 이러한 열을 모델에 포함하지 않거나, 기능 선택 기술을 사용하여 최상의 기능을 자동으로 검색하고 통계적으로 중요하지 않은 값을 제외할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-113">If only 50 of the 500 columns in the data source have information that is useful in building a model, you could just leave them out of the model, or you could use feature selection techniques to automatically discover the best features and to exclude values that are statistically insignificant.</span></span> <span data-ttu-id="7c29c-114">기능 선택은 중요하지 않은 데이터가 너무 많거나, 매우 중요한 데이터가 너무 적은 두 가지 문제를 해결하는 데 도움이 됩니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-114">Feature selection helps solve the twin problems of having too much data that is of little value, or having too little data that is of high value.</span></span>  
  
## <a name="feature-selection-in-analysis-services-data-mining"></a><span data-ttu-id="7c29c-115">Analysis Services 데이터 마이닝의 기능 선택</span><span class="sxs-lookup"><span data-stu-id="7c29c-115">Feature Selection in Analysis Services Data Mining</span></span>  
 <span data-ttu-id="7c29c-116">일반적으로 기능 선택은 [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)]에서 자동으로 수행되며 각 알고리즘에는 기능 축소를 지능형으로 적용하기 위한 기본 기술 집합이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-116">Usually, feature selection is performed automatically in [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)], and each algorithm has a set of default techniques for intelligently applying feature reduction.</span></span> <span data-ttu-id="7c29c-117">기능 선택은 모델에 사용될 가능성이 가장 높은 특성을 데이터 세트에서 자동으로 선택하기 위해 모델을 학습하기 전에 항상 수행됩니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-117">Feature selection is always performed before the model is trained, to automatically choose the attributes in a dataset that are most likely to be used in the model.</span></span> <span data-ttu-id="7c29c-118">하지만 기능 선택 동작에 영향을 주는 매개 변수를 수동으로 설정할 수도 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-118">However, you can also manually set parameters to influence feature selection behavior.</span></span>  
  
 <span data-ttu-id="7c29c-119">일반적으로 기능 선택은 각 특성에 대한 점수를 계산한 다음 점수가 가장 높은 특성만 선택하는 방식으로 작동합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-119">In general, feature selection works by calculating a score for each attribute, and then selecting only the attributes that have the best scores.</span></span> <span data-ttu-id="7c29c-120">최고 점수에 대한 임계값도 조정할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-120">You can also adjust the threshold for the top scores.</span></span> [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)]<span data-ttu-id="7c29c-121">에서는 이러한 점수를 계산하기 위한 여러 방법을 제공하며 특정 모델에 적용되는 정확한 방법은 다음 요소에 따라 결정됩니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-121">provides multiple methods for calculating these scores, and the exact method that is applied in any model depends on these factors:</span></span>  
  
-   <span data-ttu-id="7c29c-122">모델에 사용되는 알고리즘</span><span class="sxs-lookup"><span data-stu-id="7c29c-122">The algorithm used in your model</span></span>  
  
-   <span data-ttu-id="7c29c-123">특성의 데이터 형식</span><span class="sxs-lookup"><span data-stu-id="7c29c-123">The data type of the attribute</span></span>  
  
-   <span data-ttu-id="7c29c-124">모델에 설정할 수 있는 모든 매개 변수</span><span class="sxs-lookup"><span data-stu-id="7c29c-124">Any parameters that you may have set on your model</span></span>  
  
 <span data-ttu-id="7c29c-125">기능 선택은 열의 입력, 예측 가능한 특성 또는 상태에 적용됩니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-125">Feature selection is applied to inputs, predictable attributes, or to states in a column.</span></span> <span data-ttu-id="7c29c-126">기능 선택에 대한 평가가 완료되면 알고리즘이 선택한 특성 및 상태만 모델 작성 프로세스에 포함되며 예측에 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-126">When scoring for feature selection is complete, only the attributes and states that the algorithm selects are included in the model-building process and can be used for prediction.</span></span> <span data-ttu-id="7c29c-127">기능 선택에 대한 임계값과 일치하지 않는 예측 가능한 특성을 선택할 경우 이 특성을 예측에 사용할 수 있지만 예측은 모델에 있는 글로벌 통계만을 기준으로 수행됩니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-127">If you choose a predictable attribute that does not meet the threshold for feature selection the attribute can still be used for prediction, but the predictions will be based solely on the global statistics that exist in the model.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="7c29c-128">기능 선택은 모델에 사용된 열에만 영향을 미치며 마이닝 구조 스토리지에는 영향을 미치지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-128">Feature selection affects only the columns that are used in the model, and has no effect on storage of the mining structure.</span></span> <span data-ttu-id="7c29c-129">마이닝 모델에서 제외된 열은 계속 구조에서 사용할 수 있으며 마이닝 구조 열의 데이터는 캐시됩니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-129">The columns that you leave out of the mining model are still available in the structure, and data in the mining structure columns will be cached.</span></span>  
  
### <a name="definition-of-feature-selection-methods"></a><span data-ttu-id="7c29c-130">기능 선택 방법 정의</span><span class="sxs-lookup"><span data-stu-id="7c29c-130">Definition of Feature Selection Methods</span></span>  
 <span data-ttu-id="7c29c-131">기능 선택을 구현하는 방법에는 여러 가지가 있는데 작업 중인 데이터의 형식과 분석을 위해 선택한 알고리즘에 따라 이 방법이 달라집니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-131">There are many ways to implement feature selection, depending on the type of data that you are working with and the algorithm that you choose for analysis.</span></span> <span data-ttu-id="7c29c-132">SQL Server Analysis Services는 널리 사용되고 잘 알려진 특성 점수 매기기 메서드를 여러 개 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-132">SQL Server Analysis Services provides several popular and well-established methods for scoring attributes.</span></span> <span data-ttu-id="7c29c-133">알고리즘 또는 데이터 집합에 적용되는 방법은 데이터 형식 및 열 사용법에 따라 다릅니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-133">The method that is applied in any algorithm or data set depends on the data types, and the column usage.</span></span>  
  
 <span data-ttu-id="7c29c-134">*흥미도* 점수는 이진이 아닌 연속 숫자 데이터를 포함하는 열의 특성에 순위를 매기고 정렬하는 데 사용됩니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-134">The *interestingness* score is used to rank and sort attributes in columns that contain nonbinary continuous numeric data.</span></span>  
  
 <span data-ttu-id="7c29c-135">불연속 데이터와 분할된 데이터가 들어 있는 열에는*Shannon Entropy* 및 두 가지 *Bayesian* 점수를 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-135">*Shannon's entropy* and two *Bayesian* scores are available for columns that contain discrete and discretized data.</span></span> <span data-ttu-id="7c29c-136">하지만 모델에 연속 열이 포함된 경우 일관성을 보장하기 위해 흥미도 점수를 사용하여 모든 입력 열을 평가합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-136">However, if the model contains any continuous columns, the interestingness score will be used to assess all input columns, to ensure consistency.</span></span>  
  
 <span data-ttu-id="7c29c-137">다음 섹션에서는 기능 선택의 각 방법에 대해 설명합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-137">The following section describes each method of feature selection.</span></span>  
  
#### <a name="interestingness-score"></a><span data-ttu-id="7c29c-138">흥미도 점수</span><span class="sxs-lookup"><span data-stu-id="7c29c-138">Interestingness score</span></span>  
 <span data-ttu-id="7c29c-139">유용한 정보를 제공하는 기능은 흥미롭습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-139">A feature is interesting if it tells you some useful piece of information.</span></span> <span data-ttu-id="7c29c-140">유용한 항목에 대 한 정의는 시나리오에 따라 달라 지므로 데이터 마이닝 업계는 *흥미도*을 측정 하는 다양 한 방법을 개발 했습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-140">Because the definition of what is useful varies depending on the scenario, the data mining industry has developed various ways to measure *interestingness*.</span></span> <span data-ttu-id="7c29c-141">예를 들어 *새로 움* 은 이상 값 검색에서 흥미롭습니다. 그러나 긴밀 하 게 관련 된 항목 또는 *판별 가중치*사이에서 판별 하는 기능이 분류에 더 적합할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-141">For example, *novelty* might be interesting in outlier detection, but the ability to discriminate between closely related items, or *discriminating weight*, might be more interesting for classification.</span></span>  
  
 <span data-ttu-id="7c29c-142">SQL Server Analysis Services에서 사용 되는 흥미도의 측정값은 *엔트로피 기반*입니다. 즉, 임의 분포를 가진 특성의 엔트로피는 높고 정보 이점은 낮습니다. 따라서 이러한 특성은 별로 흥미롭습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-142">The measure of interestingness that is used in SQL Server Analysis Services is *entropy-based*, meaning that attributes with random distributions have higher entropy and lower information gain; therefore, such attributes are less interesting.</span></span> <span data-ttu-id="7c29c-143">특정 특성에 대한 Entropy는 다음과 같이 다른 모든 특성의 Entropy와 비교됩니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-143">The entropy for any particular attribute is compared to the entropy of all other attributes, as follows:</span></span>  
  
 <span data-ttu-id="7c29c-144">Interestingness(Attribute) = - (m - Entropy(Attribute)) \* (m - Entropy(Attribute))</span><span class="sxs-lookup"><span data-stu-id="7c29c-144">Interestingness(Attribute) = - (m - Entropy(Attribute)) \* (m - Entropy(Attribute))</span></span>  
  
 <span data-ttu-id="7c29c-145">중앙 Entropy 또는 m은 전체 기능 집합의 Entropy를 의미합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-145">Central entropy, or m, means the entropy of the entire feature set.</span></span> <span data-ttu-id="7c29c-146">중앙 Entropy에서 대상 특성의 Entropy를 뺀 값으로 특성이 제공하는 정보량을 평가할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-146">By subtracting the entropy of the target attribute from the central entropy, you can assess how much information the attribute provides.</span></span>  
  
 <span data-ttu-id="7c29c-147">이 점수는 기본적으로 이진이 아닌 연속 숫자 데이터가 열에 포함되어 있는 경우 항상 사용됩니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-147">This score is used by default whenever the column contains nonbinary continuous numeric data.</span></span>  
  
#### <a name="shannons-entropy"></a><span data-ttu-id="7c29c-148">Shannon Entropy</span><span class="sxs-lookup"><span data-stu-id="7c29c-148">Shannon's Entropy</span></span>  
 <span data-ttu-id="7c29c-149">Shannon Entropy는 특정 결과에 대한 불규칙 변수의 불확실성을 측정합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-149">Shannon's entropy measures the uncertainty of a random variable for a particular outcome.</span></span> <span data-ttu-id="7c29c-150">예를 들어 동전 던지기에 대한 Entropy는 동전 앞면이 나올 확률에 대한 함수로 표시할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-150">For example, the entropy of a coin toss can be represented as a function of the probability of it coming up heads.</span></span>  
  
 <span data-ttu-id="7c29c-151">Analysis Services에서는 다음 수식을 사용하여 Shannon Entropy를 계산합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-151">Analysis Services uses the following formula to calculate Shannon's entropy:</span></span>  
  
 <span data-ttu-id="7c29c-152">H(X) = -  P(xi) log(P(xi))</span><span class="sxs-lookup"><span data-stu-id="7c29c-152">H(X) = -∑ P(xi) log(P(xi))</span></span>  
  
 <span data-ttu-id="7c29c-153">이 점수 매기기 메서드는 불연속 특성과 불연속화된 특성에 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-153">This scoring method is available for discrete and discretized attributes.</span></span>  
  
#### <a name="bayesian-with-k2-prior"></a><span data-ttu-id="7c29c-154">Bayesian with K2 Prior</span><span class="sxs-lookup"><span data-stu-id="7c29c-154">Bayesian with K2 Prior</span></span>  
 <span data-ttu-id="7c29c-155">Analysis Services에서는 Bayesian 네트워크를 기반으로 하는 두 개의 기능 선택 점수를 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-155">Analysis Services provides two feature selection scores that are based on Bayesian networks.</span></span> <span data-ttu-id="7c29c-156">Bayesian 네트워크는 상태 및 상태 전환에 대한 *방향* 또는 *비순환* 그래프로, 항상 현재 상태 앞에 오는 상태도 있고 현재 상태 뒤에 오는 상태도 있으며 그래프가 반복되거나 순환되지 않음을 의미합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-156">A Bayesian network is a *directed* or *acyclic* graph of states and transitions between states, meaning that some states are always prior to the current state, some states are posterior, and the graph does not repeat or loop.</span></span> <span data-ttu-id="7c29c-157">정의에 따라 Bayesian 네트워크에서는 사전 지식을 활용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-157">By definition, Bayesian networks allow the use of prior knowledge.</span></span> <span data-ttu-id="7c29c-158">그러나 이후 상태에 대한 확률을 계산하는 데 사용할 이전 상태가 어떤 것인가는 알고리즘 설계, 성능 및 정확도에 중요합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-158">However, the question of which prior states to use in calculating probabilities of later states is important for algorithm design, performance, and accuracy.</span></span>  
  
 <span data-ttu-id="7c29c-159">Bayesian 네트워크를 통한 학습을 위해 고안된 K2 알고리즘은 Cooper와 Herskovits가 개발하였으며 데이터 마이닝에 자주 사용됩니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-159">The K2 algorithm for learning from a Bayesian network was developed by Cooper and Herskovits and is often used in data mining.</span></span> <span data-ttu-id="7c29c-160">K2 알고리즘은 확장 가능하며 여러 변수를 분석할 수 있지만 입력으로 사용되는 변수의 정렬을 필요로 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-160">It is scalable and can analyze multiple variables, but requires ordering on variables used as input.</span></span> <span data-ttu-id="7c29c-161">자세한 내용은 [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885) (Chickering, Geiger 및 Heckerman 공저)를 참조하십시오.</span><span class="sxs-lookup"><span data-stu-id="7c29c-161">For more information, see [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885) by Chickering, Geiger, and Heckerman.</span></span>  
  
 <span data-ttu-id="7c29c-162">이 점수 매기기 메서드는 불연속 특성과 불연속화된 특성에 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-162">This scoring method is available for discrete and discretized attributes.</span></span>  
  
#### <a name="bayesian-dirichlet-equivalent-with-uniform-prior"></a><span data-ttu-id="7c29c-163">Bayesian Dirichlet Equivalent with Uniform Prior</span><span class="sxs-lookup"><span data-stu-id="7c29c-163">Bayesian Dirichlet Equivalent with Uniform Prior</span></span>  
 <span data-ttu-id="7c29c-164">데이터 세트가 제공된 경우 BDE(Bayesian Dirichlet Equivalent) 점수도 Bayesian 분석을 사용하여 네트워크를 평가합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-164">The Bayesian Dirichlet Equivalent (BDE) score also uses Bayesian analysis to evaluate a network given a dataset.</span></span> <span data-ttu-id="7c29c-165">BDE 점수 매기기 메서드는 Heckerman이 개발했으며 Cooper와 Herskovits가 개발한 BD 메트릭을 기반으로 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-165">The BDE scoring method was developed by Heckerman and is based on the BD metric developed by Cooper and Herskovits.</span></span> <span data-ttu-id="7c29c-166">Dirichlet 분포는 네트워크에 있는 각 변수에 대한 조건부 확률을 설명하는 다차원 분포로 학습에 유용한 속성을 많이 포함합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-166">The Dirichlet distribution is a multinomial distribution that describes the conditional probability of each variable in the network, and has many properties that are useful for learning.</span></span>  
  
 <span data-ttu-id="7c29c-167">BDEU(Bayesian Dirichlet Equivalent with Uniform Prior) 메서드는 이전 상태에 대한 고정 분포 및 균일 분포를 만드는 데 수학 상수가 사용되는 Dirichlet 분포의 특수한 사례를 가정합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-167">The Bayesian Dirichlet Equivalent with Uniform Prior (BDEU) method assumes a special case of the Dirichlet distribution, in which a mathematical constant is used to create a fixed or uniform distribution of prior states.</span></span> <span data-ttu-id="7c29c-168">BDE 점수도 데이터가 등가 구조를 판별할 수 없음을 의미하는 가능성 등가를 가정합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-168">The BDE score also assumes likelihood equivalence, which means that the data cannot be expected to discriminate equivalent structures.</span></span> <span data-ttu-id="7c29c-169">다시 말해서 If A Then B에 대한 점수가 If B Then A에 대한 점수와 동일한 경우 데이터를 기반으로 구조를 구별할 수 없으며 인과 관계를 추론할 수 없습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-169">In other words, if the score for If A Then B is the same as the score for If B Then A, the structures cannot be distinguished based on the data, and causation cannot be inferred.</span></span>  
  
 <span data-ttu-id="7c29c-170">Bayesian 네트워크 및 이러한 점수 매기기 메서드의 구현에 대한 자세한 내용은 [Bayesian 네트워크 학습(Learning Bayesian Networks)](https://go.microsoft.com/fwlink/?LinkId=105885)을 참조하십시오.</span><span class="sxs-lookup"><span data-stu-id="7c29c-170">For more information about Bayesian networks and the implementation of these scoring methods, see [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885).</span></span>  
  
### <a name="feature-selection-methods-used-by-analysis-services-algorithms"></a><span data-ttu-id="7c29c-171">Analysis Services 알고리즘에서 사용하는 기능 선택 방법</span><span class="sxs-lookup"><span data-stu-id="7c29c-171">Feature Selection Methods used by Analysis Services Algorithms</span></span>  
 <span data-ttu-id="7c29c-172">다음 표에서는 기능 선택을 지원하는 알고리즘, 알고리즘에서 사용하는 기능 선택 방법 및 기능 선택 동작을 제어하기 위해 설정하는 매개 변수를 보여 줍니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-172">The following table lists the algorithms that support feature selection, the feature selection methods used by the algorithm, and the parameters that you set to control feature selection behavior:</span></span>  
  
|<span data-ttu-id="7c29c-173">알고리즘</span><span class="sxs-lookup"><span data-stu-id="7c29c-173">Algorithm</span></span>|<span data-ttu-id="7c29c-174">분석 방법</span><span class="sxs-lookup"><span data-stu-id="7c29c-174">Method of analysis</span></span>|<span data-ttu-id="7c29c-175">주석</span><span class="sxs-lookup"><span data-stu-id="7c29c-175">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="7c29c-176">Naive Bayes</span><span class="sxs-lookup"><span data-stu-id="7c29c-176">Naive Bayes</span></span>|<span data-ttu-id="7c29c-177">Shannon Entropy</span><span class="sxs-lookup"><span data-stu-id="7c29c-177">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="7c29c-178">Bayesian with K2 Prior</span><span class="sxs-lookup"><span data-stu-id="7c29c-178">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="7c29c-179">Bayesian Dirichlet with uniform prior(기본값)</span><span class="sxs-lookup"><span data-stu-id="7c29c-179">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="7c29c-180">Microsoft Naïve Bayes 알고리즘은 불연속 특성 또는 불연속화된 특성을 허용하므로 흥미도 점수를 사용할 수 없습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-180">The Microsoft Naïve Bayes algorithm accepts only discrete or discretized attributes; therefore, it cannot use the interestingness score.</span></span><br /><br /> <span data-ttu-id="7c29c-181">이 알고리즘에 대한 자세한 내용은 [Microsoft Naive Bayes Algorithm Technical Reference](microsoft-naive-bayes-algorithm-technical-reference.md)를 참조하십시오.</span><span class="sxs-lookup"><span data-stu-id="7c29c-181">For more information about this algorithm, see [Microsoft Naive Bayes Algorithm Technical Reference](microsoft-naive-bayes-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="7c29c-182">의사 결정 트리</span><span class="sxs-lookup"><span data-stu-id="7c29c-182">Decision trees</span></span>|<span data-ttu-id="7c29c-183">흥미도 점수</span><span class="sxs-lookup"><span data-stu-id="7c29c-183">Interestingness score</span></span><br /><br /> <span data-ttu-id="7c29c-184">Shannon Entropy</span><span class="sxs-lookup"><span data-stu-id="7c29c-184">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="7c29c-185">Bayesian with K2 Prior</span><span class="sxs-lookup"><span data-stu-id="7c29c-185">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="7c29c-186">Bayesian Dirichlet with uniform prior(기본값)</span><span class="sxs-lookup"><span data-stu-id="7c29c-186">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="7c29c-187">이진이 아닌 연속 값이 열에 포함되어 있는 경우 일관성을 보장하기 위해 모든 열에 흥미도 점수가 사용됩니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-187">If any columns contain non-binary continuous values, the interestingness score is used for all columns, to ensure consistency.</span></span> <span data-ttu-id="7c29c-188">그렇지 않으면 기본 기능 선택 방법이 사용되거나 모델을 만들 때 지정한 방법이 사용됩니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-188">Otherwise, the default feature selection method is used, or the method that you specified when you created the model.</span></span><br /><br /> <span data-ttu-id="7c29c-189">이 알고리즘에 대한 자세한 내용은 [Microsoft Decision Trees Algorithm Technical Reference](microsoft-decision-trees-algorithm-technical-reference.md)를 참조하십시오.</span><span class="sxs-lookup"><span data-stu-id="7c29c-189">For more information about this algorithm, see [Microsoft Decision Trees Algorithm Technical Reference](microsoft-decision-trees-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="7c29c-190">신경망</span><span class="sxs-lookup"><span data-stu-id="7c29c-190">Neural network</span></span>|<span data-ttu-id="7c29c-191">흥미도 점수</span><span class="sxs-lookup"><span data-stu-id="7c29c-191">Interestingness score</span></span><br /><br /> <span data-ttu-id="7c29c-192">Shannon Entropy</span><span class="sxs-lookup"><span data-stu-id="7c29c-192">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="7c29c-193">Bayesian with K2 Prior</span><span class="sxs-lookup"><span data-stu-id="7c29c-193">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="7c29c-194">Bayesian Dirichlet with uniform prior(기본값)</span><span class="sxs-lookup"><span data-stu-id="7c29c-194">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="7c29c-195">데이터에 연속 열이 포함된 경우 Microsoft 신경망 알고리즘에서는 Bayesian 및 Entropy 기반 방법을 모두 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-195">The Microsoft Neural Networks algorithm can use both Bayesian and entropy-based methods, as long as the data contains continuous columns.</span></span><br /><br /> <span data-ttu-id="7c29c-196">이 알고리즘에 대한 자세한 내용은 [Microsoft Neural Network Algorithm Technical Reference](microsoft-neural-network-algorithm-technical-reference.md)를 참조하십시오.</span><span class="sxs-lookup"><span data-stu-id="7c29c-196">For more information about this algorithm, see [Microsoft Neural Network Algorithm Technical Reference](microsoft-neural-network-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="7c29c-197">로지스틱 회귀</span><span class="sxs-lookup"><span data-stu-id="7c29c-197">Logistic regression</span></span>|<span data-ttu-id="7c29c-198">흥미도 점수</span><span class="sxs-lookup"><span data-stu-id="7c29c-198">Interestingness score</span></span><br /><br /> <span data-ttu-id="7c29c-199">Shannon Entropy</span><span class="sxs-lookup"><span data-stu-id="7c29c-199">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="7c29c-200">Bayesian with K2 Prior</span><span class="sxs-lookup"><span data-stu-id="7c29c-200">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="7c29c-201">Bayesian Dirichlet with uniform prior(기본값)</span><span class="sxs-lookup"><span data-stu-id="7c29c-201">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="7c29c-202">Microsoft 로지스틱 회귀 알고리즘이 Microsoft 신경망 알고리즘을 기반으로 하지만 로지스틱 회귀 모델을 사용자 지정하여 기능 선택 동작을 제어할 수 없습니다. 따라서 기능 선택은 항상 기본적으로 특성에 가장 적합한 방법으로 설정됩니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-202">Although the Microsoft Logistic Regression algorithm is based on the Microsoft Neural Network algorithm, you cannot customize logistic regression models to control feature selection behavior; therefore, feature selection always default to the method that is most appropriate for the attribute.</span></span><br /><br /> <span data-ttu-id="7c29c-203">모든 특성이 불연속 특성 또는 불연속화된 특성인 경우 기본값은 BDEU입니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-203">If all attributes are discrete or discretized, the default is BDEU.</span></span><br /><br /> <span data-ttu-id="7c29c-204">이 알고리즘에 대한 자세한 내용은 [Microsoft Logistic Regression Algorithm Technical Reference](microsoft-logistic-regression-algorithm-technical-reference.md)를 참조하십시오.</span><span class="sxs-lookup"><span data-stu-id="7c29c-204">For more information about this algorithm, see [Microsoft Logistic Regression Algorithm Technical Reference](microsoft-logistic-regression-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="7c29c-205">Clustering</span><span class="sxs-lookup"><span data-stu-id="7c29c-205">Clustering</span></span>|<span data-ttu-id="7c29c-206">흥미도 점수</span><span class="sxs-lookup"><span data-stu-id="7c29c-206">Interestingness score</span></span>|<span data-ttu-id="7c29c-207">Microsoft 클러스터링 알고리즘은 불연속 데이터 또는 불연속화된 데이터를 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-207">The Microsoft Clustering algorithm can use discrete or discretized data.</span></span> <span data-ttu-id="7c29c-208">그러나 각 특성의 점수가 거리로 계산되고 연속 숫자로 표현되기 때문에 흥미도 점수를 사용해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-208">However, because the score of each attribute is calculated as a distance and is represented as a continuous number, the interestingness score must be used.</span></span><br /><br /> <span data-ttu-id="7c29c-209">이 알고리즘에 대한 자세한 내용은 [Microsoft Clustering Algorithm Technical Reference](microsoft-clustering-algorithm-technical-reference.md)를 참조하십시오.</span><span class="sxs-lookup"><span data-stu-id="7c29c-209">For more information about this algorithm, see [Microsoft Clustering Algorithm Technical Reference](microsoft-clustering-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="7c29c-210">선형 회귀</span><span class="sxs-lookup"><span data-stu-id="7c29c-210">Linear regression</span></span>|<span data-ttu-id="7c29c-211">흥미도 점수</span><span class="sxs-lookup"><span data-stu-id="7c29c-211">Interestingness score</span></span>|<span data-ttu-id="7c29c-212">Microsoft 선형 회귀 알고리즘은 연속 열만 지원하므로 흥미도 점수만 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-212">The Microsoft Linear Regression algorithm can only use the interestingness score, because it only supports continuous columns.</span></span><br /><br /> <span data-ttu-id="7c29c-213">이 알고리즘에 대한 자세한 내용은 [Microsoft Linear Regression Algorithm Technical Reference](microsoft-linear-regression-algorithm-technical-reference.md)를 참조하십시오.</span><span class="sxs-lookup"><span data-stu-id="7c29c-213">For more information about this algorithm, see [Microsoft Linear Regression Algorithm Technical Reference](microsoft-linear-regression-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="7c29c-214">연결 규칙</span><span class="sxs-lookup"><span data-stu-id="7c29c-214">Association rules</span></span><br /><br /> <span data-ttu-id="7c29c-215">시퀀스 클러스터링</span><span class="sxs-lookup"><span data-stu-id="7c29c-215">Sequence clustering</span></span>|<span data-ttu-id="7c29c-216">사용되지 않음</span><span class="sxs-lookup"><span data-stu-id="7c29c-216">Not used</span></span>|<span data-ttu-id="7c29c-217">기능 선택은 이러한 알고리즘을 통해 호출되지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-217">Feature selection is not invoked with these algorithms.</span></span><br /><br /> <span data-ttu-id="7c29c-218">그러나 MINIMUM_SUPPORT 및 MINIMUM_PROBABILIITY 매개 변수의 값을 설정하여 알고리즘의 동작을 제어하고 필요한 경우 입력 데이터의 크기를 줄일 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-218">However, you can control the behavior of the algorithm and reduce the size of input data if necessary by setting the value of the parameters MINIMUM_SUPPORT and MINIMUM_PROBABILIITY.</span></span><br /><br /> <span data-ttu-id="7c29c-219">자세한 내용은 [Microsoft Association Algorithm Technical Reference](microsoft-association-algorithm-technical-reference.md) 및 [Microsoft Sequence Clustering Algorithm Technical Reference](microsoft-sequence-clustering-algorithm-technical-reference.md)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="7c29c-219">For more information, see [Microsoft Association Algorithm Technical Reference](microsoft-association-algorithm-technical-reference.md) and [Microsoft Sequence Clustering Algorithm Technical Reference](microsoft-sequence-clustering-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="7c29c-220">시계열</span><span class="sxs-lookup"><span data-stu-id="7c29c-220">Time series</span></span>|<span data-ttu-id="7c29c-221">사용되지 않음</span><span class="sxs-lookup"><span data-stu-id="7c29c-221">Not used</span></span>|<span data-ttu-id="7c29c-222">기능 선택은 시계열 모델에 적용되지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-222">Feature selection does not apply to time series models.</span></span><br /><br /> <span data-ttu-id="7c29c-223">이 알고리즘에 대한 자세한 내용은 [Microsoft Time Series Algorithm Technical Reference](microsoft-time-series-algorithm-technical-reference.md)(Microsoft Time Series 알고리즘 기술 참조)를 참조하십시오.</span><span class="sxs-lookup"><span data-stu-id="7c29c-223">For more information about this algorithm, see [Microsoft Time Series Algorithm Technical Reference](microsoft-time-series-algorithm-technical-reference.md).</span></span>|  
  
## <a name="feature-selection-parameters"></a><span data-ttu-id="7c29c-224">기능 선택 매개 변수</span><span class="sxs-lookup"><span data-stu-id="7c29c-224">Feature Selection Parameters</span></span>  
 <span data-ttu-id="7c29c-225">기능 선택을 지원하는 알고리즘에서 다음 매개 변수를 사용하여 기능 선택 사용 시기를 제어할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-225">In algorithms that support feature selection, you can control when feature selection is turned on by using the following parameters.</span></span> <span data-ttu-id="7c29c-226">각 알고리즘에는 허용되는 입력 수에 대한 기본값이 있지만, 이 기본값을 재정의하거나 특성 수를 지정할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-226">Each algorithm has a default value for the number of inputs that are allowed, but you can override this default and specify the number of attributes.</span></span> <span data-ttu-id="7c29c-227">이 섹션에서는 기능 선택을 관리하기 위해 제공되는 매개 변수를 나열합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-227">This section lists the parameters that are provided for managing feature selection.</span></span>  
  
#### <a name="maximum_input_attributes"></a><span data-ttu-id="7c29c-228">MAXIMUM_INPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="7c29c-228">MAXIMUM_INPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="7c29c-229">*MAXIMUM_INPUT_ATTRIBUTES* 매개 변수에서 지정한 수보다 더 많은 열이 모델에 있는 경우 알고리즘은 필요 없다고 판단되는 모든 열을 무시합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-229">If a model contains more columns than the number that is specified in the *MAXIMUM_INPUT_ATTRIBUTES* parameter, the algorithm ignores any columns that it calculates to be uninteresting.</span></span>  
  
#### <a name="maximum_output_attributes"></a><span data-ttu-id="7c29c-230">MAXIMUM_OUTPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="7c29c-230">MAXIMUM_OUTPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="7c29c-231">마찬가지로 *MAXIMUM_OUTPUT_ATTRIBUTES* 매개 변수에서 지정한 수보다 더 많은 예측 가능한 열이 모델에 있는 경우 알고리즘은 필요 없다고 판단되는 모든 열을 무시합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-231">Similarly, if a model contains more predictable columns than the number that is specified in the *MAXIMUM_OUTPUT_ATTRIBUTES* parameter, the algorithm ignores any columns that it calculates to be uninteresting.</span></span>  
  
#### <a name="maximum_states"></a><span data-ttu-id="7c29c-232">MAXIMUM_STATES</span><span class="sxs-lookup"><span data-stu-id="7c29c-232">MAXIMUM_STATES</span></span>  
 <span data-ttu-id="7c29c-233">*MAXIMUM_STATES* 매개 변수에서 지정한 수보다 더 많은 사례가 모델에 있으면 가장 인기가 없는 상태를 함께 그룹화하여 없는 것으로 처리합니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-233">If a model contains more cases than are specified in the *MAXIMUM_STATES* parameter, the least popular states are grouped together and treated as missing.</span></span> <span data-ttu-id="7c29c-234">이러한 매개 변수 중 하나를 0으로 설정할 경우 기능 선택이 해제되고 처리 시간 및 성능에 영향을 미칩니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-234">If any one of these parameters is set to 0, feature selection is turned off, affecting processing time and performance.</span></span>  
  
 <span data-ttu-id="7c29c-235">기능 선택에 대한 이러한 방법 외에도, 모델에 *모델링 플래그* 를 설정하거나 구조에 *분산 플래그* 를 설정함으로써 의미 있는 특성을 식별하거나 승격시키는 알고리즘 기능을 강화할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="7c29c-235">In addition to these methods for feature selection, you can improve the ability of the algorithm to identify or promote meaningful attributes by setting *modeling flags* on the model or by setting *distribution flags* on the structure.</span></span> <span data-ttu-id="7c29c-236">이러한 개념에 대한 자세한 내용은 [모델링 플래그&#40;데이터 마이닝&#41;](modeling-flags-data-mining.md) 및 [열 배포&#40;데이터 마이닝&#41;](column-distributions-data-mining.md)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="7c29c-236">For more information about these concepts, see [Modeling Flags &#40;Data Mining&#41;](modeling-flags-data-mining.md) and [Column Distributions &#40;Data Mining&#41;](column-distributions-data-mining.md).</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="7c29c-237">참고 항목</span><span class="sxs-lookup"><span data-stu-id="7c29c-237">See Also</span></span>  
 [<span data-ttu-id="7c29c-238">마이닝 모델 및 구조 사용자 지정</span><span class="sxs-lookup"><span data-stu-id="7c29c-238">Customize Mining Models and Structure</span></span>](customize-mining-models-and-structure.md)  
  
  
