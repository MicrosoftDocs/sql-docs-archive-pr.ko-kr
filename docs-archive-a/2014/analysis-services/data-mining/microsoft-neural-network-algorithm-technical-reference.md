---
title: Microsoft 신경망 알고리즘 기술 참조 | Microsoft Docs
ms.custom: ''
ms.date: 06/13/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- HIDDEN_NODE_RATIO parameter
- MAXIMUM_INPUT_ATTRIBUTES parameter
- HOLDOUT_PERCENTAGE parameter
- neural network algorithms [Analysis Services]
- output layer [Data Mining]
- neural networks
- MAXIMUM_OUTPUT_ATTRIBUTES parameter
- MAXIMUM_STATES parameter
- SAMPLE_SIZE parameter
- hidden layer
- hidden neurons
- input layer [Data Mining]
- activation function [Data Mining]
- Back-Propagated Delta Rule network
- neural network model [Analysis Services]
- coding [Data Mining]
- HOLDOUT_SEED parameter
ms.assetid: b8fac409-e3c0-4216-b032-364f8ea51095
author: minewiskan
ms.author: owend
ms.openlocfilehash: 3c36fd9f3446ddf36da9af7ce58259edbe84c8cf
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 08/04/2020
ms.locfileid: "87639108"
---
# <a name="microsoft-neural-network-algorithm-technical-reference"></a><span data-ttu-id="b786f-102">Microsoft 신경망 알고리즘 기술 참조</span><span class="sxs-lookup"><span data-stu-id="b786f-102">Microsoft Neural Network Algorithm Technical Reference</span></span>
  <span data-ttu-id="b786f-103">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 신경망은 최대 3 계층의 뉴런 또는 *퍼셉트론* 으로 구성된 *다중 계층 퍼셉트론*신경망( *역전파 델타 규칙 네트워크*)을 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-103">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network uses a *Multilayer Perceptron* network, also called a *Back-Propagated Delta Rule network*, composed of up to three layers of neurons, or *perceptrons*.</span></span> <span data-ttu-id="b786f-104">이러한 3개의 계층은 입력 계층, 출력 계층 및 숨겨진 계층(옵션)입니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-104">These layers are an input layer, an optional hidden layer, and an output layer.</span></span>  
  
 <span data-ttu-id="b786f-105">다중 계층 퍼셉트론 신경망에 대한 자세한 내용은 이 설명서에서 다루지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-105">A detailed discussion of Multilayer Perceptron neural networks is outside the scope of this documentation.</span></span> <span data-ttu-id="b786f-106">이 항목에서는 입력 및 출력 값을 정규화하는 데 사용되는 방법과 특성 카디널리티를 줄이는 데 사용되는 기능 선택 방법을 포함하여 알고리즘의 기본 구현에 대해 설명합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-106">This topic explains the basic implementation of the algorithm, including the method used to normalize input and output values, and feature selection methods used to reduce attribute cardinality.</span></span> <span data-ttu-id="b786f-107">또한 이 항목에서는 알고리즘의 동작을 사용자 지정하는 데 사용할 수 있는 매개 변수 및 기타 설정을 설명하고 모델 쿼리에 대한 추가 정보로 연결되는 링크를 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-107">This topic describes the parameters and other settings that can be used to customize the behavior of the algorithm, and provides links to additional information about querying the model.</span></span>  
  
## <a name="implementation-of-the-microsoft-neural-network-algorithm"></a><span data-ttu-id="b786f-108">Microsoft 신경망 알고리즘 구현</span><span class="sxs-lookup"><span data-stu-id="b786f-108">Implementation of the Microsoft Neural Network Algorithm</span></span>  
 <span data-ttu-id="b786f-109">다중 계층 퍼셉트론 신경망에서 각 뉴런은 하나 이상의 입력을 받아 하나 이상의 동일한 출력을 생성합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-109">In a Multilayer Perceptron neural network, each neuron receives one or more inputs and produces one or more identical outputs.</span></span> <span data-ttu-id="b786f-110">각 출력은 뉴런에 대한 입력 합계의 간단한 비선형 함수입니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-110">Each output is a simple non-linear function of the sum of the inputs to the neuron.</span></span> <span data-ttu-id="b786f-111">입력은 입력 계층의 노드에서 숨겨진 계층의 노드로 전달된 후 숨겨진 계층에서 출력 계층으로 전달됩니다. 계층 내 뉴런 사이에는 연결이 없습니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-111">Inputs pass forward from nodes in the input layer to nodes in the hidden layer, and then pass from the hidden layer to the output layer; there are no connections between neurons within a layer.</span></span> <span data-ttu-id="b786f-112">로지스틱 회귀 모델에서와 같이 숨겨진 계층이 포함되지 않은 경우에는 입력이 입력 계층의 노드에서 출력 계층의 노드로 직접 전달됩니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-112">If no hidden layer is included, as in a logistic regression model, inputs pass forward directly from nodes in the input layer to nodes in the output layer.</span></span>  
  
 <span data-ttu-id="b786f-113">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 신경망 알고리즘을 사용하여 생성된 신경망에는 다음과 같은 3가지 유형의 뉴런이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-113">There are three types of neurons in a neural network that is created with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm:</span></span>  
  
-   `Input neurons`  
  
 <span data-ttu-id="b786f-114">입력 뉴런은 데이터 마이닝 모델의 입력 특성 값을 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-114">Input neurons provide input attribute values for the data mining model.</span></span> <span data-ttu-id="b786f-115">불연속 입력 특성의 경우 입력 뉴런은 일반적으로 입력 특성의 단일 상태를 나타냅니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-115">For discrete input attributes, an input neuron typically represents a single state from the input attribute.</span></span> <span data-ttu-id="b786f-116">학습 데이터에 해당 특성에 대한 Null이 들어 있는 경우 누락 값도 여기에 포함됩니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-116">This includes missing values, if the training data contains nulls for that attribute.</span></span> <span data-ttu-id="b786f-117">3개 이상의 상태가 있는 불연속 입력 특성은 각 상태에 대해 하나씩의 입력 뉴런과 누락된 상태에 대한 입력 뉴런 하나(학습 데이터에 Null이 있는 경우)를 생성합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-117">A discrete input attribute that has more than two states generates one input neuron for each state, and one input neuron for a missing state, if there are any nulls in the training data.</span></span> <span data-ttu-id="b786f-118">연속 입력 특성은 두 개의 입력 뉴런, 즉 누락된 상태에 대한 뉴런 하나와 연속 특성 자체의 값에 대한 뉴런 하나를 생성합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-118">A continuous input attribute generates two input neurons: one neuron for a missing state, and one neuron for the value of the continuous attribute itself.</span></span> <span data-ttu-id="b786f-119">입력 뉴런은 하나 이상의 숨겨진 뉴런에 입력을 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-119">Input neurons provide inputs to one or more hidden neurons.</span></span>  
  
-   `Hidden neurons`  
  
 <span data-ttu-id="b786f-120">숨겨진 뉴런은 입력 뉴런에서 입력을 받아 출력 뉴런에 출력을 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-120">Hidden neurons receive inputs from input neurons and provide outputs to output neurons.</span></span>  
  
-   `Output neurons`  
  
 <span data-ttu-id="b786f-121">출력 뉴런은 데이터 마이닝 모델의 예측 가능한 특성 값을 나타냅니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-121">Output neurons represent predictable attribute values for the data mining model.</span></span> <span data-ttu-id="b786f-122">불연속 입력 특성의 경우 출력 뉴런은 일반적으로 누락된 값을 포함한 예측 가능한 특성의 단일 예측 상태를 나타냅니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-122">For discrete input attributes, an output neuron typically represents a single predicted state for a predictable attribute, including missing values.</span></span> <span data-ttu-id="b786f-123">예를 들어 예측 가능한 이진 특성은 해당 특성의 값이 있는지 여부를 나타내기 위해 누락된 상태 또는 기존 상태를 설명하는 출력 노드 하나를 생성합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-123">For example, a binary predictable attribute produces one output node that describes a missing or existing state, to indicate whether a value exists for that attribute.</span></span> <span data-ttu-id="b786f-124">예측 가능한 특성으로 사용되는 부울 열은 3개의 출력 뉴런, 즉 true 값에 대해 뉴런 하나, false 값에 대해 뉴런 하나, 그리고 누락된 상태나 기존 상태에 대해 뉴런 하나를 생성합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-124">A Boolean column that is used as a predictable attribute generates three output neurons: one neuron for a true value, one neuron for a false value, and one neuron for a missing or existing state.</span></span> <span data-ttu-id="b786f-125">3개 이상의 상태가 있는 예측 가능한 불연속 특성은 각 상태에 대해 출력 뉴런 하나와 누락된 상태 또는 기존 상태에 대해 출력 뉴런 하나를 생성합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-125">A discrete predictable attribute that has more than two states generates one output neuron for each state, and one output neuron for a missing or existing state.</span></span> <span data-ttu-id="b786f-126">예측 가능한 연속 열은 두 개의 출력 뉴런, 즉 누락된 상태 또는 기존 상태에 대해 뉴런 하나와 연속 열 자체 값에 대해 뉴런 하나를 생성합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-126">Continuous predictable columns generate two output neurons: one neuron for a missing or existing state, and one neuron for the value of the continuous column itself.</span></span> <span data-ttu-id="b786f-127">예측 가능한 열 집합을 검토하여 500개가 넘는 출력 뉴런이 생성되는 경우 [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] 에서는 마이닝 모델에 새 네트워크를 생성하여 추가 출력 뉴런을 나타냅니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-127">If more than 500 output neurons are generated by reviewing the set of predictable columns, [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] generates a new network in the mining model to represent the additional output neurons.</span></span>  
  
 <span data-ttu-id="b786f-128">뉴런은 해당 뉴런이 있는 신경망의 계층에 따라 다른 뉴런이나 다른 데이터에서 입력을 받습니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-128">A neuron receives input from other neurons, or from other data, depending on which layer of the network it is in.</span></span> <span data-ttu-id="b786f-129">입력 뉴런은 원래 데이터에서 입력을 받습니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-129">An input neuron receives inputs from the original data.</span></span> <span data-ttu-id="b786f-130">숨겨진 뉴런과 출력 뉴런은 신경망에 있는 다른 뉴런의 출력에서 입력을 받습니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-130">Hidden neurons and output neurons receive inputs from the output of other neurons in the neural network.</span></span> <span data-ttu-id="b786f-131">입력은 뉴런 간에 관계를 설정하며 이러한 관계는 특정 사례 집합에 대한 분석 경로 역할을 합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-131">Inputs establish relationships between neurons, and the relationships serve as a path of analysis for a specific set of cases.</span></span>  
  
 <span data-ttu-id="b786f-132">각 입력에는 *가중치*라는 값이 할당되어 있으며 이 값은 숨겨진 뉴런 또는 출력 뉴런에 대한 특정 입력의 관련성 또는 중요도를 설명합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-132">Each input has a value assigned to it, called the *weight*, which describes the relevance or importance of that particular input to the hidden neuron or the output neuron.</span></span> <span data-ttu-id="b786f-133">입력에 할당된 가중치가 클수록 해당 입력 값의 관련성 또는 중요도도 큽니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-133">The greater the weight that is assigned to an input, the more relevant or important the value of that input.</span></span> <span data-ttu-id="b786f-134">가중치는 음수가 될 수 있으며 이 경우 입력이 특정 뉴런을 활성화하는 것이 아니라 차단할 수 있음을 의미합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-134">Weights can be negative, which implies that the input can inhibit, rather than activate, a specific neuron.</span></span> <span data-ttu-id="b786f-135">특정 뉴런에 대한 입력의 중요도를 강조하기 위해 각 입력 값에 가중치를 곱합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-135">The value of each input is multiplied by the weight to emphasize the importance of an input for a specific neuron.</span></span> <span data-ttu-id="b786f-136">음수 가중치의 경우 값에 가중치를 곱하면 중요도가 낮아집니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-136">For negative weights, the effect of multiplying the value by the weight is to deemphasize the importance.</span></span>  
  
 <span data-ttu-id="b786f-137">각 뉴런에는 *활성화 함수*라는 간단한 비선형 함수가 할당되어 있으며 이 함수는 신경망 계층에 대한 특정 뉴런의 관련성 또는 중요도를 설명합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-137">Each neuron has a simple non-linear function assigned to it, called the *activation function*, which describes the relevance or importance of a particular neuron to that layer of a neural network.</span></span> <span data-ttu-id="b786f-138">숨겨진 뉴런은 활성화 함수로 *쌍곡 탄젠트* 함수(tanh)를 사용하는 반면 출력 뉴런은 활성화 함수로 *시그모이드* 함수를 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-138">Hidden neurons use a *hyperbolic tangent* function (tanh) for their activation function, whereas output neurons use a *sigmoid* function for activation.</span></span> <span data-ttu-id="b786f-139">두 함수는 모두 신경망이 입력 뉴런과 출력 뉴런 간의 비선형 관계를 모델링할 수 있도록 하는 비선형 연속 함수입니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-139">Both functions are nonlinear, continuous functions that allow the neural network to model nonlinear relationships between input and output neurons.</span></span>  
  
### <a name="training-neural-networks"></a><span data-ttu-id="b786f-140">신경망 학습</span><span class="sxs-lookup"><span data-stu-id="b786f-140">Training Neural Networks</span></span>  
 <span data-ttu-id="b786f-141">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 신경망 알고리즘을 사용하는 데이터 마이닝 모델의 학습에는 여러 단계가 필요합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-141">Several steps are involved in training a data mining model that uses the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span> <span data-ttu-id="b786f-142">이러한 단계는 알고리즘 매개 변수에 지정한 값에 따라 큰 영향을 받습니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-142">These steps are heavily influenced by the values that you specify for the algorithm parameters.</span></span>  
  
 <span data-ttu-id="b786f-143">알고리즘은 먼저 데이터 원본에서 학습 데이터를 평가 및 추출합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-143">The algorithm first evaluates and extracts training data from the data source.</span></span> <span data-ttu-id="b786f-144">*홀드아웃 데이터*라는 학습 데이터의 비율은 네트워크의 정확도를 평가하는 데 사용하기 위해 예약됩니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-144">A percentage of the training data, called the *holdout data*, is reserved for use in assessing the accuracy of the network.</span></span> <span data-ttu-id="b786f-145">학습 프로세스 중 네트워크는 학습 데이터에 대한 각 반복 후에 즉시 평가됩니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-145">Throughout the training process, the network is evaluated immediately after each iteration through the training data.</span></span> <span data-ttu-id="b786f-146">정확도가 더 이상 높아지지 않으면 학습 프로세스가 중지됩니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-146">When the accuracy no longer increases, the training process is stopped.</span></span>  
  
 <span data-ttu-id="b786f-147">*SAMPLE_SIZE* 및 *HOLDOUT_PERCENTAGE* 매개 변수의 값은 학습 데이터에서 샘플링할 사례 수와 홀드아웃 데이터로 예약할 사례 수를 결정하는 데 사용됩니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-147">The values of the *SAMPLE_SIZE* and *HOLDOUT_PERCENTAGE* parameters are used to determine the number of cases to sample from the training data and the number of cases to be put aside for the holdout data.</span></span> <span data-ttu-id="b786f-148">*HOLDOUT_SEED* 매개 변수 값은 홀드아웃 데이터로 예약할 개별 사례를 임의로 결정하는 데 사용됩니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-148">The value of the *HOLDOUT_SEED* parameter is used to randomly determine the individual cases to be put aside for the holdout data.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="b786f-149">이러한 알고리즘 매개 변수는 마이닝 구조에 적용되어 테스트 데이터 집합을 정의하는 HOLDOUT_SIZE 및 HOLDOUT_SEED 속성과는 다릅니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-149">These algorithm parameters are different from the HOLDOUT_SIZE and HOLDOUT_SEED properties, which are applied to a mining structure to define a testing data set.</span></span>  
  
 <span data-ttu-id="b786f-150">그런 다음 알고리즘은 마이닝 모델이 지원하는 네트워크의 수 및 복잡도를 확인합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-150">The algorithm next determines the number and complexity of the networks that the mining model supports.</span></span> <span data-ttu-id="b786f-151">마이닝 모델이 예측에만 사용되는 특성을 하나 이상 포함하면 이러한 모든 특성을 나타내는 단일 네트워크가 생성됩니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-151">If the mining model contains one or more attributes that are used only for prediction, the algorithm creates a single network that represents all such attributes.</span></span> <span data-ttu-id="b786f-152">마이닝 모델이 입력 및 예측에 모두 사용되는 특성을 하나 이상 포함하면 알고리즘 공급자는 각 특성에 대해 네트워크를 하나씩 생성합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-152">If the mining model contains one or more attributes that are used for both input and prediction, the algorithm provider constructs a network for each attribute.</span></span>  
  
 <span data-ttu-id="b786f-153">불연속 값이 포함된 입력 및 예측 가능한 특성의 경우 각 입력 또는 출력 뉴런이 각각 단일 상태를 나타냅니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-153">For input and predictable attributes that have discrete values, each input or output neuron respectively represents a single state.</span></span> <span data-ttu-id="b786f-154">연속 값이 포함된 입력 및 예측 가능한 특성의 경우에는 각 입력 또는 출력 뉴런이 각각 특성 값의 범위 및 분포를 나타냅니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-154">For input and predictable attributes that have continuous values, each input or output neuron respectively represents the range and distribution of values for the attribute.</span></span> <span data-ttu-id="b786f-155">두 경우 모두에서 지원되는 최대 상태 수는 *MAXIMUM_STATES* 알고리즘 매개 변수 값에 따라 달라집니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-155">The maximum number of states that is supported in either case depends on the value of the *MAXIMUM_STATES* algorithm parameter.</span></span> <span data-ttu-id="b786f-156">특정 특성의 상태 수가 *MAXIMUM_STATES* 알고리즘 매개 변수 값을 초과하면 해당 특성에 대해 가장 많이 사용되거나 해당 특성과 관련성이 가장 높은 상태가 최대값을 초과하지 않는 한도 내에서 선택되고 나머지 상태는 분석을 위해 누락된 값으로 그룹화됩니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-156">If the number of states for a specific attribute exceeds the value of the *MAXIMUM_STATES* algorithm parameter, the most popular or relevant states for that attribute are chosen, up to the maximum number of states allowed, and the remaining states are grouped as missing values for the purposes of analysis.</span></span>  
  
 <span data-ttu-id="b786f-157">그런 다음 알고리즘은 *HIDDEN_NODE_RATIO* 매개 변수 값을 사용하여 숨겨진 계층에 대해 만들 초기 뉴런 수를 결정합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-157">The algorithm then uses the value of the *HIDDEN_NODE_RATIO* parameter when determining the initial number of neurons to create for the hidden layer.</span></span> <span data-ttu-id="b786f-158">*HIDDEN_NODE_RATIO* 를 0으로 설정하여 알고리즘이 마이닝 모델에 대해 생성하는 네트워크에서 숨겨진 계층의 생성을 방지하고 신경망을 로지스틱 회귀 분석으로 처리할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-158">You can set *HIDDEN_NODE_RATIO* to 0 to prevent the creation of a hidden layer in the networks that the algorithm generates for the mining model, to treat the neural network as a logistic regression.</span></span>  
  
 <span data-ttu-id="b786f-159">알고리즘 공급자는 이전에 예약된 학습 데이터 집합을 사용하고 *일괄 학습*이라는 프로세스를 통해 홀드아웃 데이터에 있는 각 사례의 실제 알려진 값을 네트워크의 예측과 비교하여 네트워크 전반의 모든 입력 가중치를 반복하여 동시에 평가합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-159">The algorithm provider iteratively evaluates the weight for all inputs across the network at the same time, by taking the set of training data that was reserved earlier and comparing the actual known value for each case in the holdout data with the network's prediction, in a process known as *batch learning*.</span></span> <span data-ttu-id="b786f-160">알고리즘은 전체 학습 데이터 집합을 평가한 후 각 뉴런의 예측 값 및 실제 값을 검토합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-160">After the algorithm has evaluated the entire set of training data, the algorithm reviews the predicted and actual value for each neuron.</span></span> <span data-ttu-id="b786f-161">이때 오류 정도(있는 경우)를 계산하고 *역전파*라는 프로세스를 통해 출력 뉴런에서 입력 뉴런으로 반대 방향으로 작업하여 해당 뉴런에 대한 입력과 연결된 가중치를 조정합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-161">The algorithm calculates the degree of error, if any, and adjusts the weights that are associated with the inputs for that neuron, working backward from output neurons to input neurons in a process known as *backpropagation*.</span></span> <span data-ttu-id="b786f-162">그런 다음 전체 학습 데이터 집합에 대해 이 프로세스를 반복합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-162">The algorithm then repeats the process over the entire set of training data.</span></span> <span data-ttu-id="b786f-163">알고리즘에 다양한 가중치 및 출력 뉴런을 사용할 수 있으므로 입력에 대한 가중치 할당 및 평가를 위해 켤레 경사도 알고리즘을 사용하여 학습 프로세스를 진행할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-163">Because the algorithm can support many weights and output neurons, the conjugate gradient algorithm is used to guide the training process for assigning and evaluating weights for inputs.</span></span> <span data-ttu-id="b786f-164">켤레 경사도 알고리즘에 대한 자세한 내용은 이 설명서에서 다루지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-164">A discussion of the conjugate gradient algorithm is outside the scope of this documentation.</span></span>  
  
### <a name="feature-selection"></a><span data-ttu-id="b786f-165">기능 선택</span><span class="sxs-lookup"><span data-stu-id="b786f-165">Feature Selection</span></span>  
 <span data-ttu-id="b786f-166">입력 특성의 수가 *MAXIMUM_INPUT_ATTRIBUTES* 매개 변수 값보다 크고 예측 가능한 특성의 수가 *MAXIMUM_OUTPUT_ATTRIBUTES* 매개 변수 값보다 크면 마이닝 모델에 포함된 네트워크의 복잡도를 낮추기 위해 기능 선택 알고리즘이 사용됩니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-166">If the number of input attributes is greater than the value of the *MAXIMUM_INPUT_ATTRIBUTES* parameter, or if the number of predictable attributes is greater than the value of the *MAXIMUM_OUTPUT_ATTRIBUTES* parameter, a feature selection algorithm is used to reduce the complexity of the networks that are included in the mining model.</span></span> <span data-ttu-id="b786f-167">기능 선택은 통계상 모델과 가장 관련성이 높은 특성만 남겨 입력 또는 예측 가능한 특성 수를 줄입니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-167">Feature selection reduces the number of input or predictable attributes to those that are most statistically relevant to the model.</span></span>  
  
 <span data-ttu-id="b786f-168">모든 [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] 데이터 마이닝 알고리즘에서는 자동으로 기능 선택을 사용하여 분석을 향상시키고 처리 로드를 줄입니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-168">Feature selection is used automatically by all [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] data mining algorithms to improve analysis and reduce processing load.</span></span> <span data-ttu-id="b786f-169">신경망 모델의 기능 선택에 사용되는 방법은 특성의 데이터 형식에 따라 달라집니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-169">The method used for feature selection in neural network models depends on the data type of the attribute.</span></span> <span data-ttu-id="b786f-170">다음 표에서는 참조를 위해 신경망 모델에 사용되는 기능 선택 방법과 신경망 알고리즘을 기반으로 하는 로지스틱 회귀 알고리즘에 사용되는 기능 선택 방법을 보여 줍니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-170">For reference, the following table shows the feature selection methods used for neural network models, and also shows the feature selection methods used for the Logistic Regression algorithm, which is based on the Neural Network algorithm.</span></span>  
  
|<span data-ttu-id="b786f-171">알고리즘</span><span class="sxs-lookup"><span data-stu-id="b786f-171">Algorithm</span></span>|<span data-ttu-id="b786f-172">분석 방법</span><span class="sxs-lookup"><span data-stu-id="b786f-172">Method of analysis</span></span>|<span data-ttu-id="b786f-173">주석</span><span class="sxs-lookup"><span data-stu-id="b786f-173">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="b786f-174">신경망</span><span class="sxs-lookup"><span data-stu-id="b786f-174">Neural Network</span></span>|<span data-ttu-id="b786f-175">흥미도 점수</span><span class="sxs-lookup"><span data-stu-id="b786f-175">Interestingness score</span></span><br /><br /> <span data-ttu-id="b786f-176">Shannon Entropy</span><span class="sxs-lookup"><span data-stu-id="b786f-176">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="b786f-177">Bayesian with K2 Prior</span><span class="sxs-lookup"><span data-stu-id="b786f-177">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="b786f-178">Bayesian Dirichlet with uniform prior(기본값)</span><span class="sxs-lookup"><span data-stu-id="b786f-178">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="b786f-179">데이터에 연속 열이 포함된 경우 신경망 알고리즘에서는 Entropy 기반 및 Bayesian 점수 매기기 방법을 모두 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-179">The Neural Networks algorithm can use both entropy-based and Bayesian scoring methods, as long as the data contains continuous columns.</span></span><br /><br /> <span data-ttu-id="b786f-180">기본.</span><span class="sxs-lookup"><span data-stu-id="b786f-180">Default.</span></span>|  
|<span data-ttu-id="b786f-181">로지스틱 회귀</span><span class="sxs-lookup"><span data-stu-id="b786f-181">Logistic Regression</span></span>|<span data-ttu-id="b786f-182">흥미도 점수</span><span class="sxs-lookup"><span data-stu-id="b786f-182">Interestingness score</span></span><br /><br /> <span data-ttu-id="b786f-183">Shannon Entropy</span><span class="sxs-lookup"><span data-stu-id="b786f-183">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="b786f-184">Bayesian with K2 Prior</span><span class="sxs-lookup"><span data-stu-id="b786f-184">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="b786f-185">Bayesian Dirichlet with uniform prior(기본값)</span><span class="sxs-lookup"><span data-stu-id="b786f-185">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="b786f-186">이 알고리즘에 매개 변수를 전달하여 기능 선택 동작을 제어할 수 없기 때문에 기본값이 사용됩니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-186">Because you cannot pass a parameter to this algorithm to control feature election behavior, the defaults are used.</span></span> <span data-ttu-id="b786f-187">그러므로 모든 특성이 불연속 또는 분할된 특성일 경우 기본값은 BDEU입니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-187">Therefore, if all attributes are discrete or discretized, the default is BDEU.</span></span>|  
  
 <span data-ttu-id="b786f-188">신경망 모델의 기능 선택을 제어하는 알고리즘 매개 변수는 MAXIMUM_INPUT_ATTRIBUTES, MAXIMUM_OUTPUT_ATTRIBUTES 및 MAXIMUM_STATES입니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-188">The algorithm parameters that control feature selection for a neural network model are MAXIMUM_INPUT_ATTRIBUTES, MAXIMUM_OUTPUT_ATTRIBUTES, and MAXIMUM_STATES.</span></span> <span data-ttu-id="b786f-189">HIDDEN_NODE_RATIO 매개 변수를 설정하여 숨겨진 계층 수를 제어할 수도 있습니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-189">You can also control the number of hidden layers by setting the HIDDEN_NODE_RATIO parameter.</span></span>  
  
### <a name="scoring-methods"></a><span data-ttu-id="b786f-190">점수 매기기 방법</span><span class="sxs-lookup"><span data-stu-id="b786f-190">Scoring Methods</span></span>  
 <span data-ttu-id="b786f-191">*점수 매기기* 는 정규화의 한 종류로서, 신경망 모델 학습 컨텍스트에서는 불연속 텍스트 레이블과 같은 값을 네트워크에서 다른 유형의 입력과 비교하고 가중치를 적용할 수 있는 값으로 변환하는 프로세스를 의미합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-191">*Scoring* is a kind of normalization, which in the context of training a neural network model means the process of converting a value, such as a discrete text label, into a value that can be compared with other types of inputs and weighted in the network.</span></span> <span data-ttu-id="b786f-192">예를 들어 한 입력 특성이 Gender이고 가능한 값은 Male 및 Female이며 다른 입력 특성은 Income이고 값 범위가 가변적인 경우, 각 특성의 값은 직접 비교할 수 없으므로 가중치를 계산할 수 있도록 공통적인 배율로 인코딩되어야 합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-192">For example, if one input attribute is Gender and the possible values are Male and Female, and another input attribute is Income, with a variable range of values, the values for each attribute are not directly comparable, and therefore must be encoded to a common scale so that the weights can be computed.</span></span> <span data-ttu-id="b786f-193">점수 매기기는 이러한 입력을 숫자 값, 특히 확률 범위로 정규화하는 프로세스입니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-193">Scoring is the process of normalizing such inputs to numeric values: specifically, to a probability range.</span></span> <span data-ttu-id="b786f-194">정규화에 사용되는 함수는 극단적인 값으로 인해 분석 결과가 왜곡되지 않도록 입력 값을 동일한 배율로 보다 균일하게 분산시키는 데도 유용합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-194">The functions used for normalization also help to distribute input value more evenly on a uniform scale so that extreme values do not distort the results of analysis.</span></span>  
  
 <span data-ttu-id="b786f-195">신경망의 출력도 인코딩됩니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-195">Outputs of the neural network are also encoded.</span></span> <span data-ttu-id="b786f-196">출력(즉, 예측)에 대한 단일 대상이 있거나 예측에만 사용되고 입력에는 사용되지 않는 여러 대상이 있는 경우에는 모델에서 단일 네트워크를 만들기 때문에 값을 정규화할 필요가 없을 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-196">When there is a single target for output (that is, prediction), or multiple targets that are used for prediction only and not for input, the model create a single network and it might not seem necessary to normalize the values.</span></span> <span data-ttu-id="b786f-197">그러나 입력 및 예측에 여러 개의 특성이 사용된 경우에는 모델에서 여러 네트워크를 만들어야 하므로 모든 값을 정규화해야 하며 네트워크에서 출력이 나갈 때 출력도 인코딩해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-197">However, if multiple attributes are used for input and prediction, the model must create multiple networks; therefore, all values must be normalized, and the outputs too must be encoded as they exit the network.</span></span>  
  
 <span data-ttu-id="b786f-198">입력에 대한 인코딩은 학습 사례에 있는 각 불연속 값을 더하고 그 합에 가중치를 곱한 값을 기반으로 합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-198">Encoding for inputs is based on summing each discrete value in the training cases, and multiplying that value by its weight.</span></span> <span data-ttu-id="b786f-199">이를 *가중치 합*이라고 하며 이 값은 숨겨진 계층의 활성화 함수에 전달됩니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-199">This is called a *weighted sum*, which is passed to the activation function in the hidden layer.</span></span> <span data-ttu-id="b786f-200">인코딩에는 다음과 같이 z 점수가 사용됩니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-200">A z-score is used for encoding, as follows:</span></span>  
  
 <span data-ttu-id="b786f-201">**불연속 값**</span><span class="sxs-lookup"><span data-stu-id="b786f-201">**Discrete values**</span></span>  
  
 <span data-ttu-id="b786f-202">μ = p-상태의 사전 확률</span><span class="sxs-lookup"><span data-stu-id="b786f-202">μ = p - the prior probability of a state</span></span>  
  
 <span data-ttu-id="b786f-203">StdDev  = sqrt(p(1-p))</span><span class="sxs-lookup"><span data-stu-id="b786f-203">StdDev  = sqrt(p(1-p))</span></span>  
  
 <span data-ttu-id="b786f-204">**연속 값**</span><span class="sxs-lookup"><span data-stu-id="b786f-204">**Continuous values**</span></span>  
  
 <span data-ttu-id="b786f-205">값 있음 = 1-μ/σ</span><span class="sxs-lookup"><span data-stu-id="b786f-205">Value present= 1 - μ/σ</span></span>  
  
 <span data-ttu-id="b786f-206">기존 값 없음 =-μ/σ</span><span class="sxs-lookup"><span data-stu-id="b786f-206">No existing value= -μ/σ</span></span>  
  
 <span data-ttu-id="b786f-207">값이 인코딩된 후 입력은 네트워크 가장자리를 가중치로 하여 가중치 합을 구하는 과정을 거칩니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-207">After the values have been encoded, the inputs go through weighted summing, with network edges as weights.</span></span>  
  
 <span data-ttu-id="b786f-208">출력에 대한 인코딩에는 예측에 매우 유용한 속성이 있는 시그모이드 함수가 사용됩니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-208">Encoding for outputs uses the sigmoid function, which has properties that make it very useful for prediction.</span></span> <span data-ttu-id="b786f-209">이러한 속성 중 하나는 원래 값의 배율 조정 방식과 값이 양수인지 음수인지에 관계없이 이 함수의 출력이 항상 확률을 0에서 1 사이의 값이므로 확률을 예상하는 데 적합하다는 것입니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-209">One such property is that, regardless of how the original values are scaled, and regardless of whether values are negative or positive, the output of this function is always a value between 0 and 1, which is suited for estimating probabilities.</span></span> <span data-ttu-id="b786f-210">또 다른 유용한 속성은 시그모이드 함수에 다듬기 효과가 있으므로 값이 굴곡 지점에서 멀리 벗어날수록 값의 확률은 천천히 0 또는 1에 가까워진다는 것입니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-210">Another useful property is that the sigmoid function has a smoothing effect, so that as values move farther away from point of inflection, the probability for the value moves towards 0 or 1, but slowly.</span></span>  
  
## <a name="customizing-the-neural-network-algorithm"></a><span data-ttu-id="b786f-211">신경망 알고리즘 사용자 지정</span><span class="sxs-lookup"><span data-stu-id="b786f-211">Customizing the Neural Network Algorithm</span></span>  
 <span data-ttu-id="b786f-212">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 신경망 알고리즘은 결과 마이닝 모델의 동작, 성능 및 정확도에 영향을 주는 여러 매개 변수를 지원합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-212">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm supports several parameters that affect the behavior, performance, and accuracy of the resulting mining model.</span></span> <span data-ttu-id="b786f-213">열에 모델링 플래그를 설정하여 모델의 데이터 처리 방식을 수정하거나 분산 플래그를 설정하여 열 내의 값이 처리되는 방식을 지정할 수도 있습니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-213">You can also modify the way that the model processes data by setting modeling flags on columns, or by setting distribution flags to specify how values within the column are handled.</span></span>  
  
### <a name="setting-algorithm-parameters"></a><span data-ttu-id="b786f-214">알고리즘 매개 변수 설정</span><span class="sxs-lookup"><span data-stu-id="b786f-214">Setting Algorithm Parameters</span></span>  
 <span data-ttu-id="b786f-215">다음 표에서는 Microsoft 신경망 알고리즘에서 사용할 수 있는 매개 변수에 대해 설명합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-215">The following table describes the parameters that can be used with the Microsoft Neural Network algorithm.</span></span>  
  
 <span data-ttu-id="b786f-216">HIDDEN_NODE_RATIO</span><span class="sxs-lookup"><span data-stu-id="b786f-216">HIDDEN_NODE_RATIO</span></span>  
 <span data-ttu-id="b786f-217">입력 및 출력 뉴런에 대한 숨겨진 뉴런의 비율을 지정합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-217">Specifies the ratio of hidden neurons to input and output neurons.</span></span> <span data-ttu-id="b786f-218">다음 수식에서는 숨겨진 계층의 초기 뉴런 수를 결정합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-218">The following formula determines the initial number of neurons in the hidden layer:</span></span>  
  
 <span data-ttu-id="b786f-219">HIDDEN_NODE_RATIO \* SQRT(총 입력 뉴런 \* 총 출력 뉴런)</span><span class="sxs-lookup"><span data-stu-id="b786f-219">HIDDEN_NODE_RATIO \* SQRT(Total input neurons \* Total output neurons)</span></span>  
  
 <span data-ttu-id="b786f-220">기본값은 4.0입니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-220">The default value is 4.0.</span></span>  
  
 <span data-ttu-id="b786f-221">HOLDOUT_PERCENTAGE</span><span class="sxs-lookup"><span data-stu-id="b786f-221">HOLDOUT_PERCENTAGE</span></span>  
 <span data-ttu-id="b786f-222">홀드아웃 오류를 계산하는 데 사용되는 학습 데이터 내의 사례 비율을 지정합니다. 이 비율은 마이닝 모델 학습 중 중지 조건의 일부로 사용됩니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-222">Specifies the percentage of cases within the training data used to calculate the holdout error, which is used as part of the stopping criteria while training the mining model.</span></span>  
  
 <span data-ttu-id="b786f-223">기본값은 30입니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-223">The default value is 30.</span></span>  
  
 <span data-ttu-id="b786f-224">HOLDOUT_SEED</span><span class="sxs-lookup"><span data-stu-id="b786f-224">HOLDOUT_SEED</span></span>  
 <span data-ttu-id="b786f-225">알고리즘이 홀드아웃 데이터를 임의로 결정할 때 난수 생성기의 초기값으로 사용할 숫자를 지정합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-225">Specifies a number that is used to seed the pseudo-random generator when the algorithm randomly determines the holdout data.</span></span> <span data-ttu-id="b786f-226">이 매개 변수를 0으로 설정하면 알고리즘은 마이닝 모델의 이름을 기반으로 초기값을 생성하여 다시 처리하는 동안 모델 콘텐츠가 동일하게 유지되도록 합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-226">If this parameter is set to 0, the algorithm generates the seed based on the name of the mining model, to guarantee that the model content remains the same during reprocessing.</span></span>  
  
 <span data-ttu-id="b786f-227">기본값은 0입니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-227">The default value is 0.</span></span>  
  
 <span data-ttu-id="b786f-228">MAXIMUM_INPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="b786f-228">MAXIMUM_INPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="b786f-229">기능 선택을 사용하기 전에 알고리즘에 제공할 수 있는 최대 입력 특성 수를 결정합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-229">Determines the maximum number of input attributes that can be supplied to the algorithm before feature selection is employed.</span></span> <span data-ttu-id="b786f-230">이 값을 0으로 설정하면 입력 특성에 대해 기능 선택을 사용할 수 없습니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-230">Setting this value to 0 disables feature selection for input attributes.</span></span>  
  
 <span data-ttu-id="b786f-231">기본값은 255입니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-231">The default value is 255.</span></span>  
  
 <span data-ttu-id="b786f-232">MAXIMUM_OUTPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="b786f-232">MAXIMUM_OUTPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="b786f-233">기능 선택을 사용하기 전에 알고리즘에 제공할 수 있는 최대 출력 특성 수를 결정합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-233">Determines the maximum number of output attributes that can be supplied to the algorithm before feature selection is employed.</span></span> <span data-ttu-id="b786f-234">이 값을 0으로 설정하면 출력 특성에 대해 기능 선택을 사용할 수 없습니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-234">Setting this value to 0 disables feature selection for output attributes.</span></span>  
  
 <span data-ttu-id="b786f-235">기본값은 255입니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-235">The default value is 255.</span></span>  
  
 <span data-ttu-id="b786f-236">MAXIMUM_STATES</span><span class="sxs-lookup"><span data-stu-id="b786f-236">MAXIMUM_STATES</span></span>  
 <span data-ttu-id="b786f-237">알고리즘이 지원하는 특성당 최대 불연속 상태 수를 지정합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-237">Specifies the maximum number of discrete states per attribute that is supported by the algorithm.</span></span> <span data-ttu-id="b786f-238">특정 특성의 상태 수가 이 매개 변수에 지정한 수보다 크면 알고리즘은 해당 특성에 대해 가장 많이 사용되는 상태를 사용하고 나머지 상태를 누락된 것으로 처리합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-238">If the number of states for a specific attribute is greater than the number that is specified for this parameter, the algorithm uses the most popular states for that attribute and treats the remaining states as missing.</span></span>  
  
 <span data-ttu-id="b786f-239">기본값은 100입니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-239">The default value is 100.</span></span>  
  
 <span data-ttu-id="b786f-240">SAMPLE_SIZE</span><span class="sxs-lookup"><span data-stu-id="b786f-240">SAMPLE_SIZE</span></span>  
 <span data-ttu-id="b786f-241">모델의 학습에 사용되는 사례 수를 지정합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-241">Specifies the number of cases to be used to train the model.</span></span> <span data-ttu-id="b786f-242">이 알고리즘은 HOLDOUT_PERCENTAGE 매개 변수에 의해 지정된 홀드아웃 데이터에 포함되지 않은 총 사례 수의 비율이나 이 숫자 중 더 작은 값을 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-242">The algorithm uses either this number or the percentage of total of cases not included in the holdout data as specified by the HOLDOUT_PERCENTAGE parameter, whichever value is smaller.</span></span>  
  
 <span data-ttu-id="b786f-243">즉, HOLDOUT_PERCENTAGE를 30으로 설정하면 알고리즘은 이 매개 변수 값이나 총 사례 수의 70%에 해당하는 값 중 더 작은 값을 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-243">In other words, if HOLDOUT_PERCENTAGE is set to 30, the algorithm will use either the value of this parameter, or a value equal to 70 percent of the total number of cases, whichever is smaller.</span></span>  
  
 <span data-ttu-id="b786f-244">기본값은 10000입니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-244">The default value is 10000.</span></span>  
  
### <a name="modeling-flags"></a><span data-ttu-id="b786f-245">모델링 플래그</span><span class="sxs-lookup"><span data-stu-id="b786f-245">Modeling Flags</span></span>  
 <span data-ttu-id="b786f-246">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 신경망 알고리즘에서 지원되는 모델링 플래그는 다음과 같습니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-246">The following modeling flags are supported for use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span>  
  
 <span data-ttu-id="b786f-247">NOT NULL</span><span class="sxs-lookup"><span data-stu-id="b786f-247">NOT NULL</span></span>  
 <span data-ttu-id="b786f-248">열에 null이 포함될 수 없음을 나타냅니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-248">Indicates that the column cannot contain a null.</span></span> <span data-ttu-id="b786f-249">따라서 Analysis Services가 모델 학습 중 Null을 발견할 경우 오류가 발생합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-249">An error will result if Analysis Services encounters a null during model training.</span></span>  
  
 <span data-ttu-id="b786f-250">마이닝 구조 열에 적용됩니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-250">Applies to mining structure columns.</span></span>  
  
 <span data-ttu-id="b786f-251">MODEL_EXISTENCE_ONLY</span><span class="sxs-lookup"><span data-stu-id="b786f-251">MODEL_EXISTENCE_ONLY</span></span>  
 <span data-ttu-id="b786f-252">모델에서 특성 값이 존재하는지 누락되었는지만 고려해야 함을 나타냅니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-252">Indicates that the model should only consider whether a value exists for the attribute or if a value is missing.</span></span> <span data-ttu-id="b786f-253">정확한 값은 고려하지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-253">The exact value does not matter.</span></span>  
  
 <span data-ttu-id="b786f-254">마이닝 모델 열에 적용됩니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-254">Applies to mining model columns.</span></span>  
  
### <a name="distribution-flags"></a><span data-ttu-id="b786f-255">분산 플래그</span><span class="sxs-lookup"><span data-stu-id="b786f-255">Distribution Flags</span></span>  
 <span data-ttu-id="b786f-256">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 신경망 알고리즘에서 지원되는 분산 플래그는 다음과 같습니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-256">The following distribution flags are supported for use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span> <span data-ttu-id="b786f-257">이 플래그는 모델에 대해서만 힌트로 사용됩니다. 알고리즘에서는 다른 분산을 발견할 경우 힌트에 제공된 분산이 아니라 발견된 분산을 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-257">The flags are used as hints to the model only; if the algorithm detects a different distribution it will use the found distribution, not the distribution provided in the hint.</span></span>  
  
 <span data-ttu-id="b786f-258">보통</span><span class="sxs-lookup"><span data-stu-id="b786f-258">Normal</span></span>  
 <span data-ttu-id="b786f-259">열 내의 값이 정규 가우스 분포를 나타내는 것처럼 처리되어야 함을 나타냅니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-259">Indicates that values within the column should be treated as though they represent the normal, or Gaussian, distribution.</span></span>  
  
 <span data-ttu-id="b786f-260">Uniform</span><span class="sxs-lookup"><span data-stu-id="b786f-260">Uniform</span></span>  
 <span data-ttu-id="b786f-261">열 내의 값이 일정하게 분산된 것처럼 처리되어야 함을 나타냅니다. 즉, 값의 확률은 거의 같으며 값의 총 수에 대한 함수여야 함을 나타냅니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-261">Indicates that values within the column should be treated as though they are distributed uniformly; that is, the probability of any value is roughly equal, and is a function of the total number of values.</span></span>  
  
 <span data-ttu-id="b786f-262">Log Normal</span><span class="sxs-lookup"><span data-stu-id="b786f-262">Log Normal</span></span>  
 <span data-ttu-id="b786f-263">열 내의 값이 *log normal* 곡선에 따라 분산된 것처럼 처리되어야 함을 나타냅니다. 즉, 값의 로그가 일정하게 분산되어야 함을 나타냅니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-263">Indicates that values within the column should be treated as though distributed according to the *log normal* curve, which means that the logarithm of the values is distributed normally.</span></span>  
  
## <a name="requirements"></a><span data-ttu-id="b786f-264">요구 사항</span><span class="sxs-lookup"><span data-stu-id="b786f-264">Requirements</span></span>  
 <span data-ttu-id="b786f-265">신경망 모델은 하나 이상의 입력 열과 하나의 출력 열을 포함해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-265">A neural network model must contain at least one input column and one output column.</span></span>  
  
### <a name="input-and-predictable-columns"></a><span data-ttu-id="b786f-266">입력 열과 예측 가능한 열</span><span class="sxs-lookup"><span data-stu-id="b786f-266">Input and Predictable Columns</span></span>  
 <span data-ttu-id="b786f-267">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 신경망 알고리즘은 다음 표에 나열된 특정 입력 열과 예측 가능한 열을 지원합니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-267">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm supports the specific input columns and predictable columns that are listed in the following table.</span></span>  
  
|<span data-ttu-id="b786f-268">열</span><span class="sxs-lookup"><span data-stu-id="b786f-268">Column</span></span>|<span data-ttu-id="b786f-269">내용 유형</span><span class="sxs-lookup"><span data-stu-id="b786f-269">Content types</span></span>|  
|------------|-------------------|  
|<span data-ttu-id="b786f-270">입력 특성</span><span class="sxs-lookup"><span data-stu-id="b786f-270">Input attribute</span></span>|<span data-ttu-id="b786f-271">Continuous, Cyclical, Discrete, Discretized, Key, Table 및 Ordered</span><span class="sxs-lookup"><span data-stu-id="b786f-271">Continuous, Cyclical, Discrete, Discretized, Key, Table, and Ordered</span></span>|  
|<span data-ttu-id="b786f-272">예측 가능한 특성</span><span class="sxs-lookup"><span data-stu-id="b786f-272">Predictable attribute</span></span>|<span data-ttu-id="b786f-273">Continuous, Cyclical, Discrete, Discretized 및 Ordered</span><span class="sxs-lookup"><span data-stu-id="b786f-273">Continuous, Cyclical, Discrete, Discretized, and Ordered</span></span>|  
  
> [!NOTE]  
>  <span data-ttu-id="b786f-274">Cyclical  및 Ordered  내용 유형이 지원되기는 하지만 알고리즘은 해당 유형을 불연속 값으로 처리하고 특수한 처리를 수행하지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="b786f-274">Cyclical and Ordered content types are supported, but the algorithm treats them as discrete values and does not perform special processing.</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="b786f-275">참고 항목</span><span class="sxs-lookup"><span data-stu-id="b786f-275">See Also</span></span>  
 <span data-ttu-id="b786f-276">[Microsoft 신경망 알고리즘](microsoft-neural-network-algorithm.md) </span><span class="sxs-lookup"><span data-stu-id="b786f-276">[Microsoft Neural Network Algorithm](microsoft-neural-network-algorithm.md) </span></span>  
 <span data-ttu-id="b786f-277">[신경망 모델에 대 한 마이닝 모델 콘텐츠 &#40;Analysis Services 데이터 마이닝&#41;](mining-model-content-for-neural-network-models-analysis-services-data-mining.md) </span><span class="sxs-lookup"><span data-stu-id="b786f-277">[Mining Model Content for Neural Network Models &#40;Analysis Services - Data Mining&#41;](mining-model-content-for-neural-network-models-analysis-services-data-mining.md) </span></span>  
 [<span data-ttu-id="b786f-278">신경망 모델 쿼리 예제</span><span class="sxs-lookup"><span data-stu-id="b786f-278">Neural Network Model Query Examples</span></span>](neural-network-model-query-examples.md)  
  
  
