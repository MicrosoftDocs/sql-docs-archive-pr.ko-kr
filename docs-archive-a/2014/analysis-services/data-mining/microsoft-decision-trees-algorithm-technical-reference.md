---
title: Microsoft 의사 결정 트리 알고리즘 기술 참조 | Microsoft Docs
ms.custom: ''
ms.date: 06/13/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- MAXIMUM_INPUT_ATTRIBUTES parameter
- SPLIT_METHOD parameter
- MINIMUM_SUPPORT parameter
- MAXIMUM_OUTPUT_ATTRIBUTES parameter
- FORCED_REGRESSOR parameter
- decision tree algorithms [Analysis Services]
- decision trees [Analysis Services]
- COMPLEXITY_PENALTY parameter
- SCORE_METHOD parameter
ms.assetid: 1e9f7969-0aa6-465a-b3ea-57b8d1c7a1fd
author: minewiskan
ms.author: owend
ms.openlocfilehash: 0cd0cd3100d0ed1213183815ae41f17cee3baa68
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 08/04/2020
ms.locfileid: "87652440"
---
# <a name="microsoft-decision-trees-algorithm-technical-reference"></a><span data-ttu-id="32616-102">Microsoft 의사 결정 트리 알고리즘 기술 참조</span><span class="sxs-lookup"><span data-stu-id="32616-102">Microsoft Decision Trees Algorithm Technical Reference</span></span>
  <span data-ttu-id="32616-103">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 의사 결정 트리 알고리즘은 트리를 만드는 여러 방법을 통합하며 회귀, 분류, 연결 등의 여러 분석 태스크를 지원하는 하이브리드 알고리즘입니다.</span><span class="sxs-lookup"><span data-stu-id="32616-103">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm is a hybrid algorithm that incorporates different methods for creating a tree, and supports multiple analytic tasks, including regression, classification, and association.</span></span> <span data-ttu-id="32616-104">Microsoft 의사 결정 트리 알고리즘은 불연속 특성과 연속 특성 모두의 모델링을 지원합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-104">The Microsoft Decision Trees algorithm supports modeling of both discrete and continuous attributes.</span></span>  
  
 <span data-ttu-id="32616-105">이 항목에서는 알고리즘의 구현을 설명하고, 여러 태스크에 대한 알고리즘 동작을 사용자 지정하는 방법을 설명하며, 의사 결정 트리 모델 쿼리에 대한 추가 정보로 연결되는 링크를 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-105">This topic explains the implementation of the algorithm, describes how to customize the behavior of the algorithm for different tasks, and provides links to additional information about querying decision tree models.</span></span>  
  
## <a name="implementation-of-the-decision-trees-algorithm"></a><span data-ttu-id="32616-106">의사 결정 트리 알고리즘의 구현</span><span class="sxs-lookup"><span data-stu-id="32616-106">Implementation of the Decision Trees Algorithm</span></span>  
 <span data-ttu-id="32616-107">Microsoft 의사 결정 트리 알고리즘은 모델에 대한 근사 사후 분포를 가져옴으로써 Bayesian 방법을 학습 인과 상호 작용 모델에 적용합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-107">The Microsoft Decision Trees algorithm applies the Bayesian approach to learning causal interaction models by obtaining approximate posterior distributions for the models.</span></span> <span data-ttu-id="32616-108">이 방법에 대한 자세한 내용은 Microsoft Research 사이트의 자료, [구조와 매개 변수 학습](https://go.microsoft.com/fwlink/?LinkId=237640&clcid=0x409)을 참조하십시오.</span><span class="sxs-lookup"><span data-stu-id="32616-108">For a detailed explanation of this approach, see the paper on the Microsoft Research site, by [Structure and Parameter Learning](https://go.microsoft.com/fwlink/?LinkId=237640&clcid=0x409).</span></span>  
  
 <span data-ttu-id="32616-109">학습에 필요한 *사전 지식* 의 정보 값을 평가하는 방법은 *가능성 등가*의 가정을 기반으로 합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-109">The methodology for assessing the information value of the *priors* needed for learning is based on the assumption of *likelihood equivalence*.</span></span> <span data-ttu-id="32616-110">이 가정은 조건부 독립성의 동일한 단정을 다른 방법으로 나타내는 네트워크 구조를 판별하는 데 데이터가 유용하지 않다는 가정입니다.</span><span class="sxs-lookup"><span data-stu-id="32616-110">This assumption says that data should not help to discriminate network structures that otherwise represent the same assertions of conditional independence.</span></span> <span data-ttu-id="32616-111">각 사례는 하나의 Bayesian 사전 지식 네트워크와 해당 네트워크의 신뢰성에 대한 하나의 측정값을 포함하는 것으로 가정됩니다.</span><span class="sxs-lookup"><span data-stu-id="32616-111">Each case is assumed to have a single Bayesian prior network and a single measure of confidence for that network.</span></span>  
  
 <span data-ttu-id="32616-112">알고리즘은 이러한 사전 지식 네트워크를 사용하여 현재 학습 데이터에 대해 네트워크 구조의 상대적 *사후 확률* 을 계산하고 사후 확률이 가장 높은 네트워크 구조를 식별합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-112">Using these prior networks, the algorithm then computes the relative *posterior probabilities* of network structures given the current training data, and identifies the network structures that have the highest posterior probabilities.</span></span>  
  
 <span data-ttu-id="32616-113">Microsoft 의사 결정 트리 알고리즘에서는 다양한 방법을 사용하여 최상의 트리를 컴퓨팅합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-113">The Microsoft Decision Trees algorithm uses different methods to compute the best tree.</span></span> <span data-ttu-id="32616-114">사용되는 방법은 태스크에 따라 선형 회귀, 분류 또는 연결 분석일 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-114">The method used depends on the task, which can be linear regression, classification, or association analysis.</span></span> <span data-ttu-id="32616-115">하나의 모델이 예측 가능한 여러 특성에 대한 여러 개의 트리를 포함할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-115">A single model can contain multiple trees for different predictable attributes.</span></span> <span data-ttu-id="32616-116">또한 각 트리는 데이터에 있는 특성 및 값의 수에 따라 여러 분기를 포함할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-116">Moreover, each tree can contain multiple branches, depending on how many attributes and values there are in the data.</span></span> <span data-ttu-id="32616-117">특정 모델에 작성되는 트리의 형태와 깊이는 점수 매기기 방법과 사용된 기타 매개 변수에 따라 달라집니다.</span><span class="sxs-lookup"><span data-stu-id="32616-117">The shape and depth of the tree built in a particular model depends on the scoring method and other parameters that were used.</span></span> <span data-ttu-id="32616-118">매개 변수의 변경 내용은 노드 분할 위치에도 영향을 줍니다.</span><span class="sxs-lookup"><span data-stu-id="32616-118">Changes in the parameters can also affect where the nodes split.</span></span>  
  
### <a name="building-the-tree"></a><span data-ttu-id="32616-119">트리 작성</span><span class="sxs-lookup"><span data-stu-id="32616-119">Building the Tree</span></span>  
 <span data-ttu-id="32616-120">Microsoft 의사 결정 트리 알고리즘은 가능한 입력 값 집합을 만들 때 *feature selection* 을 수행하여 가장 많은 정보를 제공하는 특성 및 값을 식별하고 매우 드물게 나타나는 값은 고려하지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-120">When the Microsoft Decision Trees algorithm creates the set of possible input values, it performs *feature selection* to identify the attributes and values that provide the most information, and removes from consideration the values that are very rare.</span></span> <span data-ttu-id="32616-121">또한 이 알고리즘은 값을 *Bin*에 그룹화하여 성능을 최적화하기 위해 한 단위로 처리할 수 있는 값 그룹을 만듭니다.</span><span class="sxs-lookup"><span data-stu-id="32616-121">The algorithm also groups values into *bins*, to create groupings of values that can be processed as a unit to optimize performance.</span></span>  
  
 <span data-ttu-id="32616-122">트리는 입력과 목표 결과 간의 상관 관계를 확인하여 작성됩니다.</span><span class="sxs-lookup"><span data-stu-id="32616-122">A tree is built by determining the correlations between an input and the targeted outcome.</span></span> <span data-ttu-id="32616-123">모든 특성의 상관 관계가 확인된 후 알고리즘은 결과를 가장 명확하게 구분하는 단일 특성을 식별합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-123">After all the attributes have been correlated, the algorithm identifies the single attribute that most cleanly separates the outcomes.</span></span> <span data-ttu-id="32616-124">최상의 구분 지점은 얻은 정보를 계산하는 수식을 사용하여 측정됩니다.</span><span class="sxs-lookup"><span data-stu-id="32616-124">This point of the best separation is measured by using an equation that calculates information gain.</span></span> <span data-ttu-id="32616-125">얻은 정보에 대한 최상의 점수가 있는 특성은 사례를 하위 집합으로 나누는 데 사용되고 하위 집합은 트리를 더 이상 분할할 수 없을 때까지 동일한 프로세스에서 재귀적으로 분석됩니다.</span><span class="sxs-lookup"><span data-stu-id="32616-125">The attribute that has the best score for information gain is used to divide the cases into subsets, which are then recursively analyzed by the same process, until the tree cannot be split any more.</span></span>  
  
 <span data-ttu-id="32616-126">얻은 정보를 계산하는 데 사용되는 정확한 수식은 알고리즘을 만들 때 설정한 매개 변수, 예측 가능한 열의 데이터 형식 및 입력의 데이터 형식에 따라 달라집니다.</span><span class="sxs-lookup"><span data-stu-id="32616-126">The exact equation used to evaluate information gain depends on the parameters set when you created the algorithm, the data type of the predictable column, and the data type of the input.</span></span>  
  
### <a name="discrete-and-continuous-inputs"></a><span data-ttu-id="32616-127">불연속 및 연속 입력</span><span class="sxs-lookup"><span data-stu-id="32616-127">Discrete and Continuous Inputs</span></span>  
 <span data-ttu-id="32616-128">예측 가능한 특성과 입력이 모두 불연속적일 경우 입력당 결과 수를 계산하려면 행렬을 만들고 해당 행렬에 있는 각 셀에 대한 점수를 생성해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-128">When the predictable attribute is discrete and the inputs are discrete, counting the outcomes per input is a matter of creating a matrix and generating scores for each cell in the matrix.</span></span>  
  
 <span data-ttu-id="32616-129">그러나 예측 가능한 특성이 불연속적이고 입력이 연속적일 경우에는 연속 열의 입력이 자동으로 분할됩니다.</span><span class="sxs-lookup"><span data-stu-id="32616-129">However, when the predictable attribute is discrete and the inputs are continuous, the input of the continuous columns are automatically discretized.</span></span> <span data-ttu-id="32616-130">기본값을 그대로 사용하고 [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] 에서 최적의 Bin 수를 찾도록 설정할 수도 있고, <xref:Microsoft.AnalysisServices.ScalarMiningStructureColumn.DiscretizationMethod%2A> 및 <xref:Microsoft.AnalysisServices.ScalarMiningStructureColumn.DiscretizationBucketCount%2A> 속성을 설정하여 연속 입력이 불연속화되는 방식을 제어할 수도 있습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-130">You can accept the default and have [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] find the optimum number of bins, or you can control the manner in which continuous inputs are discretized by setting the <xref:Microsoft.AnalysisServices.ScalarMiningStructureColumn.DiscretizationMethod%2A> and <xref:Microsoft.AnalysisServices.ScalarMiningStructureColumn.DiscretizationBucketCount%2A> properties.</span></span> <span data-ttu-id="32616-131">자세한 내용은 [마이닝 모델에서 열의 불연속화 변경](change-the-discretization-of-a-column-in-a-mining-model.md)을 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="32616-131">For more information, see [Change the Discretization of a Column in a Mining Model](change-the-discretization-of-a-column-in-a-mining-model.md).</span></span>  
  
 <span data-ttu-id="32616-132">연속 특성의 경우 알고리즘은 선형 회귀를 사용하여 의사 결정 트리의 분할 위치를 결정합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-132">For continuous attributes, the algorithm uses linear regression to determine where a decision tree splits.</span></span>  
  
 <span data-ttu-id="32616-133">예측 가능한 특성이 연속 숫자 데이터 형식일 경우 기능 선택은 출력에도 적용되어 가능한 결과 수를 줄이므로 모델을 보다 빠르게 작성할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-133">When the predictable attribute is a continuous numeric data type, feature selection is applied to the outputs as well, to reduce the possible number of outcomes and build the model faster.</span></span> <span data-ttu-id="32616-134">기능 선택의 임계값을 변경하고 그에 따라 MAXIMUM_OUTPUT_ATTRIBUTES 매개 변수를 설정하여 가능한 값의 수를 늘리거나 줄일 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-134">You can change the threshold for feature selection and thereby increase or decrease the number of possible values by setting the MAXIMUM_OUTPUT_ATTRIBUTES parameter.</span></span>  
  
 <span data-ttu-id="32616-135">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 의사 결정 트리 알고리즘에서 예측 가능한 불연속 열을 사용하는 방법은 [Bayesian 네트워크 학습: 지식 및 통계 데이터의 조합(Learning Bayesian Networks: The Combination of Knowledge and Statistical Data)](https://go.microsoft.com/fwlink/?LinkId=45963)을 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="32616-135">For a more detained explanation about how the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm works with discrete predictable columns, see [Learning Bayesian Networks: The Combination of Knowledge and Statistical Data](https://go.microsoft.com/fwlink/?LinkId=45963).</span></span> <span data-ttu-id="32616-136">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 의사 결정 트리 알고리즘에서 예측 가능한 연속 열을 사용하는 방법에 대한 자세한 내용은 [시계열 분석을 위한 자동 회귀 트리 모델](https://go.microsoft.com/fwlink/?LinkId=45966)의 부록을 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="32616-136">For more information about how the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm works with a continuous predictable column, see the appendix of [Autoregressive Tree Models for Time-Series Analysis](https://go.microsoft.com/fwlink/?LinkId=45966).</span></span>  
  
### <a name="scoring-methods-and-feature-selection"></a><span data-ttu-id="32616-137">점수 매기기 방법 및 기능 선택</span><span class="sxs-lookup"><span data-stu-id="32616-137">Scoring Methods and Feature Selection</span></span>  
 <span data-ttu-id="32616-138">Microsoft 의사 결정 트리 알고리즘에서는 얻은 정보를 평가하기 위한 Shannon's entropy, Bayesian network with K2 prior 및 Bayesian network with a uniform Dirichlet distribution of priors라는 세 개의 수식을 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-138">The Microsoft Decision Trees algorithm offers three formulas for scoring information gain: Shannon's entropy, Bayesian network with K2 prior, and Bayesian network with a uniform Dirichlet distribution of priors.</span></span> <span data-ttu-id="32616-139">세 방법 모두 데이터 마이닝 분야에서 잘 수립된 방법입니다.,</span><span class="sxs-lookup"><span data-stu-id="32616-139">All three methods are well established in the data mining field.</span></span> <span data-ttu-id="32616-140">여러 가지 매개 변수와 점수 매기기 방법을 사용해 보고 어느 것이 최상의 결과를 제공하는지 확인하는 것이 좋습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-140">We recommend that you experiment with different parameters and scoring methods to determine which provides the best results.</span></span> <span data-ttu-id="32616-141">이러한 점수 매기기 방법에 대한 자세한 내용은 [Feature Selection](../../sql-server/install/feature-selection.md)을 참조하십시오.</span><span class="sxs-lookup"><span data-stu-id="32616-141">For more information about these scoring methods, see [Feature Selection](../../sql-server/install/feature-selection.md).</span></span>  
  
 <span data-ttu-id="32616-142">모든 [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] 데이터 마이닝 알고리즘에서는 자동으로 기능 선택을 사용하여 분석을 향상시키고 처리 로드를 줄입니다.</span><span class="sxs-lookup"><span data-stu-id="32616-142">All [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] data mining algorithms automatically use feature selection to improve analysis and reduce processing load.</span></span> <span data-ttu-id="32616-143">기능 선택에 사용되는 방법은 모델을 작성하는 데 사용된 알고리즘에 따라 달라집니다.</span><span class="sxs-lookup"><span data-stu-id="32616-143">The method used for feature selection depends on the algorithm that is used to build the model.</span></span> <span data-ttu-id="32616-144">의사 결정 트리 모델의 기능 선택을 제어하는 알고리즘 매개 변수는 MAXIMUM_INPUT_ATTRIBUTES와 MAXIMUM_OUTPUT입니다.</span><span class="sxs-lookup"><span data-stu-id="32616-144">The algorithm parameters that control feature selection for a decision trees model are MAXIMUM_INPUT_ATTRIBUTES and MAXIMUM_OUTPUT.</span></span>  
  
|<span data-ttu-id="32616-145">알고리즘</span><span class="sxs-lookup"><span data-stu-id="32616-145">Algorithm</span></span>|<span data-ttu-id="32616-146">분석 방법</span><span class="sxs-lookup"><span data-stu-id="32616-146">Method of analysis</span></span>|<span data-ttu-id="32616-147">주석</span><span class="sxs-lookup"><span data-stu-id="32616-147">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="32616-148">의사 결정 트리</span><span class="sxs-lookup"><span data-stu-id="32616-148">Decision Trees</span></span>|<span data-ttu-id="32616-149">흥미도 점수</span><span class="sxs-lookup"><span data-stu-id="32616-149">Interestingness score</span></span><br /><br /> <span data-ttu-id="32616-150">Shannon Entropy</span><span class="sxs-lookup"><span data-stu-id="32616-150">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="32616-151">Bayesian with K2 Prior</span><span class="sxs-lookup"><span data-stu-id="32616-151">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="32616-152">Bayesian Dirichlet with uniform prior(기본값)</span><span class="sxs-lookup"><span data-stu-id="32616-152">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="32616-153">이진이 아닌 연속 값이 열에 포함되어 있는 경우 일관성을 보장하기 위해 모든 열에 흥미도 점수가 사용됩니다.</span><span class="sxs-lookup"><span data-stu-id="32616-153">If any columns contain non-binary continuous values, the interestingness score is used for all columns, to ensure consistency.</span></span> <span data-ttu-id="32616-154">그렇지 않을 경우 기본 방법이나 지정된 방법이 사용됩니다.</span><span class="sxs-lookup"><span data-stu-id="32616-154">Otherwise, the default or specified method is used.</span></span>|  
|<span data-ttu-id="32616-155">선형 회귀</span><span class="sxs-lookup"><span data-stu-id="32616-155">Linear Regression</span></span>|<span data-ttu-id="32616-156">흥미도 점수</span><span class="sxs-lookup"><span data-stu-id="32616-156">Interestingness score</span></span>|<span data-ttu-id="32616-157">선형 회귀는 연속 열만 지원하므로 흥미도 점수만 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-157">Linear Regression only uses interestingness, because it only supports continuous columns.</span></span>|  
  
### <a name="scalability-and-performance"></a><span data-ttu-id="32616-158">확장성 및 성능</span><span class="sxs-lookup"><span data-stu-id="32616-158">Scalability and Performance</span></span>  
 <span data-ttu-id="32616-159">분류는 중요한 데이터 마이닝 전략입니다.</span><span class="sxs-lookup"><span data-stu-id="32616-159">Classification is an important data mining strategy.</span></span> <span data-ttu-id="32616-160">일반적으로 사례를 분류하는 데 필요한 정보의 양은 입력 레코드의 수에 직접적으로 비례하여 증가합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-160">Generally, the amount of information that is needed to classify the cases grows in direct proportion to the number of input records.</span></span> <span data-ttu-id="32616-161">이로 인해 분류할 수 있는 데이터의 크기가 제한됩니다.</span><span class="sxs-lookup"><span data-stu-id="32616-161">This limits the size of the data that can be classified.</span></span> <span data-ttu-id="32616-162">Microsoft 의사 결정 트리 알고리즘에서는 다음 방법을 사용하여 이러한 문제를 해결하고 성능을 향상시키고 메모리 제한을 제거합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-162">The Microsoft Decision Trees algorithm using uses the following methods to resolve these problems, improve performance, and eliminate memory restrictions:</span></span>  
  
-   <span data-ttu-id="32616-163">기능 선택을 사용하여 특성 선택을 최적화합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-163">Feature selection to optimize the selection of attributes.</span></span>  
  
-   <span data-ttu-id="32616-164">Bayesian 점수 매기기를 사용하여 트리 증가를 제어합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-164">Bayesian scoring to control tree growth.</span></span>  
  
-   <span data-ttu-id="32616-165">연속 특성에 대한 Bin 생성을 최적화합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-165">Optimization of binning for continuous attributes.</span></span>  
  
-   <span data-ttu-id="32616-166">입력 값을 동적으로 그룹화하여 가장 중요한 값을 확인합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-166">Dynamic grouping of input values to determine the most important values.</span></span>  
  
 <span data-ttu-id="32616-167">Microsoft 의사 결정 트리 알고리즘은 빠르고 확장 가능할 뿐 아니라, 쉽게 병렬 처리할 수 있도록 디자인되었으므로 모든 프로세서가 함께 작동하여 하나의 일관된 모델을 작성합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-167">The Microsoft Decision Trees algorithm is fast and scalable, and has been designed to be easily parallelized, meaning that all processors work together to build a single, consistent model.</span></span> <span data-ttu-id="32616-168">이러한 모든 특성으로 인해 의사 결정 트리 분류자는 데이터 마이닝에 이상적인 도구입니다.</span><span class="sxs-lookup"><span data-stu-id="32616-168">The combination of these characteristics makes the decision-tree classifier an ideal tool for data mining.</span></span>  
  
 <span data-ttu-id="32616-169">성능 제한이 심각한 경우 의사 결정 트리 모델의 학습 도중 다음 방법을 사용하여 처리 시간을 개선할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-169">If performance constraints are severe, you might be able to improve processing time during the training of a decision tree model by using the following methods.</span></span> <span data-ttu-id="32616-170">그러나 이 경우 특성을 제거하여 처리 성능을 향상시키면 모델 결과가 변경되고 해당 모델이 전체 모집단을 대표하는 정도가 낮아질 수 있다는 것을 알고 있어야 합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-170">However, if you do so, be aware that eliminating attributes to improve processing performance will change the results of the model, and possibly make it less representative of the total population.</span></span>  
  
-   <span data-ttu-id="32616-171">COMPLEXITY_PENALTY 매개 변수의 값을 늘려 트리 증가를 제한합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-171">Increase the value of the COMPLEXITY_PENALTY parameter to limit tree growth.</span></span>  
  
-   <span data-ttu-id="32616-172">연결 모델의 항목 수를 제한하여 작성되는 트리 수를 제한합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-172">Limit the number of items in association models to limit the number of trees that are built.</span></span>  
  
-   <span data-ttu-id="32616-173">MINIMUM_SUPPORT 매개 변수의 값을 늘려 과잉 맞춤을 방지합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-173">Increase the value of the MINIMUM_SUPPORT parameter to avoid overfitting.</span></span>  
  
-   <span data-ttu-id="32616-174">모든 특성의 불연속 값 수를 10개 이하로 제한합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-174">Restrict the number of discrete values for any attribute to 10 or less.</span></span> <span data-ttu-id="32616-175">다른 모델에서 다른 방법으로 값을 그룹화해 볼 수도 있습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-175">You might try grouping values in different ways in different models.</span></span>  
  
    > [!NOTE]  
    >  <span data-ttu-id="32616-176">데이터 마이닝을 시작하기 전에  [!INCLUDE[ssISCurrent](../../includes/ssiscurrent-md.md)] 에서 제공되는 데이터 탐색 도구를 사용하여 데이터의 값 분포를 시각화하고 적절하게 값을 그룹화할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-176">You can use the data exploration tools available in  [!INCLUDE[ssISCurrent](../../includes/ssiscurrent-md.md)] to visualize the distribution of values in your data and group your values appropriately before beginning data mining.</span></span> <span data-ttu-id="32616-177">자세한 내용은 [데이터 프로파일링 태스크 및 뷰어](../../integration-services/control-flow/data-profiling-task-and-viewer.md)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="32616-177">For more information, see [Data Profiling Task and Viewer](../../integration-services/control-flow/data-profiling-task-and-viewer.md).</span></span> <span data-ttu-id="32616-178">[Excel 2007용 데이터 마이닝 추가 기능](https://www.microsoft.com/download/details.aspx?id=8569)을 사용하여 Microsoft Excel에서 데이터를 탐색하고 그룹화하고 레이블을 재지정할 수도 있습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-178">You can also use the [Data Mining Add-ins for Excel 2007](https://www.microsoft.com/download/details.aspx?id=8569), to explore, group and relabel data in Microsoft Excel.</span></span>  
  
## <a name="customizing-the-decision-trees-algorithm"></a><span data-ttu-id="32616-179">의사 결정 트리 알고리즘 사용자 지정</span><span class="sxs-lookup"><span data-stu-id="32616-179">Customizing the Decision Trees Algorithm</span></span>  
 <span data-ttu-id="32616-180">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 의사 결정 트리 알고리즘은 결과 마이닝 모델의 성능 및 정확도에 영향을 주는 매개 변수를 지원합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-180">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm supports parameters that affect the performance and accuracy of the resulting mining model.</span></span> <span data-ttu-id="32616-181">마이닝 모델 열이나 마이닝 구조 열에 모델링 플래그를 설정하여 데이터 처리 방식을 제어할 수도 있습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-181">You can also set modeling flags on the mining model columns or mining structure columns to control the way that data is processed.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="32616-182">Microsoft 의사 결정 트리 알고리즘은 모든 버전의 [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]에서 사용할 수 있지만 Microsoft 의사 결정 트리 알고리즘의 동작을 사용자 지정하는 고급 매개 변수는 특정 버전의 [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]에서만 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-182">The Microsoft Decision Trees algorithm is available in all editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]; however, some advanced parameters for customizing the behavior of the Microsoft Decision Trees algorithm are available for use only in specific editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)].</span></span> <span data-ttu-id="32616-183">버전에서 지원 되는 기능 목록은 [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)] [SQL Server 2012 버전에서 지 원하는 기능](https://go.microsoft.com/fwlink/?linkid=232473) 을 참조 하세요 https://go.microsoft.com/fwlink/?linkid=232473) .</span><span class="sxs-lookup"><span data-stu-id="32616-183">For a list of features that are supported by the editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)], see [Features Supported by the Editions of SQL Server 2012](https://go.microsoft.com/fwlink/?linkid=232473) (https://go.microsoft.com/fwlink/?linkid=232473).</span></span>  
  
### <a name="setting-algorithm-parameters"></a><span data-ttu-id="32616-184">알고리즘 매개 변수 설정</span><span class="sxs-lookup"><span data-stu-id="32616-184">Setting Algorithm Parameters</span></span>  
 <span data-ttu-id="32616-185">다음 표에서는 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 의사 결정 트리 알고리즘에서 사용할 수 있는 매개 변수에 대해 설명합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-185">The following table describes the parameters that you can use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm.</span></span>  
  
 <span data-ttu-id="32616-186">*COMPLEXITY_PENALTY*</span><span class="sxs-lookup"><span data-stu-id="32616-186">*COMPLEXITY_PENALTY*</span></span>  
 <span data-ttu-id="32616-187">의사 결정 트리의 증가를 제어합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-187">Controls the growth of the decision tree.</span></span> <span data-ttu-id="32616-188">낮은 값을 지정하면 분할 수가 증가되고 높은 값을 지정하면 분할 수가 감소됩니다.</span><span class="sxs-lookup"><span data-stu-id="32616-188">A low value increases the number of splits, and a high value decreases the number of splits.</span></span> <span data-ttu-id="32616-189">기본값은 다음 목록에 설명된 것과 같이 특정 모델의 특성 수에 따라 달라집니다.</span><span class="sxs-lookup"><span data-stu-id="32616-189">The default value is based on the number of attributes for a particular model, as described in the following list:</span></span>  
  
-   <span data-ttu-id="32616-190">특성 수가 1에서 9 사이인 경우 기본값은 0.5입니다.</span><span class="sxs-lookup"><span data-stu-id="32616-190">For 1 through 9 attributes, the default is 0.5.</span></span>  
  
-   <span data-ttu-id="32616-191">특성 수가 10에서 99 사이인 경우 기본값은 0.9입니다.</span><span class="sxs-lookup"><span data-stu-id="32616-191">For 10 through 99 attributes, the default is 0.9.</span></span>  
  
-   <span data-ttu-id="32616-192">특성 수가 100 이상인 경우 기본값은 0.99입니다.</span><span class="sxs-lookup"><span data-stu-id="32616-192">For 100 or more attributes, the default is 0.99.</span></span>  
  
 <span data-ttu-id="32616-193">*FORCE_REGRESSOR*</span><span class="sxs-lookup"><span data-stu-id="32616-193">*FORCE_REGRESSOR*</span></span>  
 <span data-ttu-id="32616-194">알고리즘에서 계산한 열의 중요도에 관계없이 알고리즘에서 지정된 열을 회귀 변수로 사용하도록 합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-194">Forces the algorithm to use the specified columns as regressors, regardless of the importance of the columns as calculated by the algorithm.</span></span> <span data-ttu-id="32616-195">이 매개 변수는 연속 특성을 예측하는 의사 결정 트리에만 사용됩니다.</span><span class="sxs-lookup"><span data-stu-id="32616-195">This parameter is only used for decision trees that are predicting a continuous attribute.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="32616-196">이 매개 변수를 설정하면 알고리즘에서는 해당 특성을 회귀 변수로 사용하려고 합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-196">By setting this parameter, you force the algorithm to try to use the attribute as a regressor.</span></span> <span data-ttu-id="32616-197">그러나 해당 특성이 최종 모델에서 실제로 회귀 변수로 사용되는지 여부는 분석 결과에 따라 달라집니다.</span><span class="sxs-lookup"><span data-stu-id="32616-197">However, whether the attribute is actually used as a regressor in the final model depends on the results of analysis.</span></span> <span data-ttu-id="32616-198">모델 콘텐츠를 쿼리하면 회귀 변수로 사용된 열을 확인할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-198">You can find out which columns were used as regressors by querying the model content.</span></span>  
  
 <span data-ttu-id="32616-199">[ [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)] 의 일부 버전에서만 사용 가능]</span><span class="sxs-lookup"><span data-stu-id="32616-199">[Available only in some editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)] ]</span></span>  
  
 <span data-ttu-id="32616-200">*MAXIMUM_INPUT_ATTRIBUTES*</span><span class="sxs-lookup"><span data-stu-id="32616-200">*MAXIMUM_INPUT_ATTRIBUTES*</span></span>  
 <span data-ttu-id="32616-201">기능 선택을 호출하기 전에 알고리즘이 처리할 수 있는 입력 특성 수를 정의합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-201">Defines the number of input attributes that the algorithm can handle before it invokes feature selection.</span></span>  
  
 <span data-ttu-id="32616-202">기본값은 255입니다.</span><span class="sxs-lookup"><span data-stu-id="32616-202">The default is 255.</span></span>  
  
 <span data-ttu-id="32616-203">이 값을 0으로 설정하면 기능 선택이 해제됩니다.</span><span class="sxs-lookup"><span data-stu-id="32616-203">Set this value to 0 to turn off feature selection.</span></span>  
  
 <span data-ttu-id="32616-204">[ [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]의 일부 버전에서만 사용 가능]</span><span class="sxs-lookup"><span data-stu-id="32616-204">[Available only in some editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]]</span></span>  
  
 <span data-ttu-id="32616-205">*MAXIMUM_OUTPUT_ATTRIBUTES*</span><span class="sxs-lookup"><span data-stu-id="32616-205">*MAXIMUM_OUTPUT_ATTRIBUTES*</span></span>  
 <span data-ttu-id="32616-206">기능 선택을 호출하기 전에 알고리즘이 처리할 수 있는 출력 특성 수를 정의합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-206">Defines the number of output attributes that the algorithm can handle before it invokes feature selection.</span></span>  
  
 <span data-ttu-id="32616-207">기본값은 255입니다.</span><span class="sxs-lookup"><span data-stu-id="32616-207">The default is 255.</span></span>  
  
 <span data-ttu-id="32616-208">이 값을 0으로 설정하면 기능 선택이 해제됩니다.</span><span class="sxs-lookup"><span data-stu-id="32616-208">Set this value to 0 to turn off feature selection.</span></span>  
  
 <span data-ttu-id="32616-209">[ [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]의 일부 버전에서만 사용 가능]</span><span class="sxs-lookup"><span data-stu-id="32616-209">[Available only in some editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]]</span></span>  
  
 <span data-ttu-id="32616-210">*MINIMUM_SUPPORT*</span><span class="sxs-lookup"><span data-stu-id="32616-210">*MINIMUM_SUPPORT*</span></span>  
 <span data-ttu-id="32616-211">의사 결정 트리에서 분할을 생성하는 데 필요한 최소 리프 사례 수를 결정합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-211">Determines the minimum number of leaf cases that is required to generate a split in the decision tree.</span></span>  
  
 <span data-ttu-id="32616-212">기본값은 10입니다.</span><span class="sxs-lookup"><span data-stu-id="32616-212">The default is 10.</span></span>  
  
 <span data-ttu-id="32616-213">데이터 세트가 매우 큰 경우 과잉 맞춤을 방지하기 위해 이 값을 늘려야 할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-213">You may need to increase this value if the dataset is very large, to avoid overtraining.</span></span>  
  
 <span data-ttu-id="32616-214">*SCORE_METHOD*</span><span class="sxs-lookup"><span data-stu-id="32616-214">*SCORE_METHOD*</span></span>  
 <span data-ttu-id="32616-215">분할 점수를 계산하는 데 사용되는 메서드를 결정합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-215">Determines the method that is used to calculate the split score.</span></span> <span data-ttu-id="32616-216">다음 옵션을 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-216">The following options are available:</span></span>  
  
|<span data-ttu-id="32616-217">ID</span><span class="sxs-lookup"><span data-stu-id="32616-217">ID</span></span>|<span data-ttu-id="32616-218">속성</span><span class="sxs-lookup"><span data-stu-id="32616-218">Name</span></span>|  
|--------|----------|  
|<span data-ttu-id="32616-219">1</span><span class="sxs-lookup"><span data-stu-id="32616-219">1</span></span>|<span data-ttu-id="32616-220">Entropy</span><span class="sxs-lookup"><span data-stu-id="32616-220">Entropy</span></span>|  
|<span data-ttu-id="32616-221">3</span><span class="sxs-lookup"><span data-stu-id="32616-221">3</span></span>|<span data-ttu-id="32616-222">Bayesian with K2 Prior</span><span class="sxs-lookup"><span data-stu-id="32616-222">Bayesian with K2 Prior</span></span>|  
|<span data-ttu-id="32616-223">4</span><span class="sxs-lookup"><span data-stu-id="32616-223">4</span></span>|<span data-ttu-id="32616-224">Bayesian Dirichlet Equivalent (BDE) with uniform prior</span><span class="sxs-lookup"><span data-stu-id="32616-224">Bayesian Dirichlet Equivalent (BDE) with uniform prior</span></span><br /><br /> <span data-ttu-id="32616-225">(기본값)</span><span class="sxs-lookup"><span data-stu-id="32616-225">(default)</span></span>|  
  
 <span data-ttu-id="32616-226">기본값은 4 또는 BDE입니다.</span><span class="sxs-lookup"><span data-stu-id="32616-226">The default is 4, or BDE.</span></span>  
  
 <span data-ttu-id="32616-227">이러한 점수 매기기 방법에 대한 자세한 내용은 [Feature Selection](../../sql-server/install/feature-selection.md)을 참조하십시오.</span><span class="sxs-lookup"><span data-stu-id="32616-227">For an explanation of these scoring methods, see [Feature Selection](../../sql-server/install/feature-selection.md).</span></span>  
  
 <span data-ttu-id="32616-228">*SPLIT_METHOD*</span><span class="sxs-lookup"><span data-stu-id="32616-228">*SPLIT_METHOD*</span></span>  
 <span data-ttu-id="32616-229">노드를 분할하는 데 사용되는 메서드를 결정합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-229">Determines the method that is used to split the node.</span></span> <span data-ttu-id="32616-230">다음 옵션을 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-230">The following options are available:</span></span>  
  
|<span data-ttu-id="32616-231">ID</span><span class="sxs-lookup"><span data-stu-id="32616-231">ID</span></span>|<span data-ttu-id="32616-232">속성</span><span class="sxs-lookup"><span data-stu-id="32616-232">Name</span></span>|  
|--------|----------|  
|<span data-ttu-id="32616-233">1</span><span class="sxs-lookup"><span data-stu-id="32616-233">1</span></span>|<span data-ttu-id="32616-234">**Binary:** 특성의 실제 값 수에 관계없이 트리가 두 개의 분리로 분할됨을 나타냅니다.</span><span class="sxs-lookup"><span data-stu-id="32616-234">**Binary:** Indicates that regardless of the actual number of values for the attribute, the tree should be split into two branches.</span></span>|  
|<span data-ttu-id="32616-235">2</span><span class="sxs-lookup"><span data-stu-id="32616-235">2</span></span>|<span data-ttu-id="32616-236">**Complete:** 트리에서 특성 값 수만큼의 분할을 만들 수 있음을 나타냅니다.</span><span class="sxs-lookup"><span data-stu-id="32616-236">**Complete:** Indicates that the tree can create as many splits as there are attribute values.</span></span>|  
|<span data-ttu-id="32616-237">3</span><span class="sxs-lookup"><span data-stu-id="32616-237">3</span></span>|<span data-ttu-id="32616-238">**Both:** 최상의 결과를 생성하기 위해 이진(Both) 분할을 사용할지 완전(Complete) 분할을 사용할지를 Analysis Services에서 결정할 수 있도록 지정합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-238">**Both:** Specifies that Analysis Services can determine whether a binary or complete split should be used to produce the best results.</span></span>|  
  
 <span data-ttu-id="32616-239">기본값은 3입니다.</span><span class="sxs-lookup"><span data-stu-id="32616-239">The default is 3.</span></span>  
  
### <a name="modeling-flags"></a><span data-ttu-id="32616-240">모델링 플래그</span><span class="sxs-lookup"><span data-stu-id="32616-240">Modeling Flags</span></span>  
 <span data-ttu-id="32616-241">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 의사 결정 트리 알고리즘은 다음과 같은 모델링 플래그를 지원합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-241">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm supports the following modeling flags.</span></span> <span data-ttu-id="32616-242">마이닝 구조나 마이닝 모델을 만들 경우 분석 중 각 열의 값이 처리되는 방법을 지정하기 위해 모델링 플래그를 정의합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-242">When you create the mining structure or mining model, you define modeling flags to specify how values in each column are handled during analysis.</span></span> <span data-ttu-id="32616-243">자세한 내용은 [모델링 플래그&#40;데이터 마이닝&#41;](modeling-flags-data-mining.md)를 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="32616-243">For more information, see [Modeling Flags &#40;Data Mining&#41;](modeling-flags-data-mining.md).</span></span>  
  
|<span data-ttu-id="32616-244">모델링 플래그</span><span class="sxs-lookup"><span data-stu-id="32616-244">Modeling Flag</span></span>|<span data-ttu-id="32616-245">Description</span><span class="sxs-lookup"><span data-stu-id="32616-245">Description</span></span>|  
|-------------------|-----------------|  
|<span data-ttu-id="32616-246">MODEL_EXISTENCE_ONLY</span><span class="sxs-lookup"><span data-stu-id="32616-246">MODEL_EXISTENCE_ONLY</span></span>|<span data-ttu-id="32616-247">열이 `Missing` 및 `Existing` 상태를 갖는 것으로 간주됩니다.</span><span class="sxs-lookup"><span data-stu-id="32616-247">Means that the column will be treated as having two possible states: `Missing` and `Existing`.</span></span> <span data-ttu-id="32616-248">Null은 누락 값입니다.</span><span class="sxs-lookup"><span data-stu-id="32616-248">A null is a missing value.</span></span><br /><br /> <span data-ttu-id="32616-249">마이닝 모델 열에 적용됩니다.</span><span class="sxs-lookup"><span data-stu-id="32616-249">Applies to mining model columns.</span></span>|  
|<span data-ttu-id="32616-250">NOT NULL</span><span class="sxs-lookup"><span data-stu-id="32616-250">NOT NULL</span></span>|<span data-ttu-id="32616-251">열에 null이 포함될 수 없음을 나타냅니다.</span><span class="sxs-lookup"><span data-stu-id="32616-251">Indicates that the column cannot contain a null.</span></span> <span data-ttu-id="32616-252">따라서 Analysis Services가 모델 학습 중 Null을 발견할 경우 오류가 발생합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-252">An error will result if Analysis Services encounters a null during model training.</span></span><br /><br /> <span data-ttu-id="32616-253">마이닝 구조 열에 적용됩니다.</span><span class="sxs-lookup"><span data-stu-id="32616-253">Applies to mining structure columns.</span></span>|  
  
### <a name="regressors-in-decision-tree-models"></a><span data-ttu-id="32616-254">의사 결정 트리 모델의 회귀 변수</span><span class="sxs-lookup"><span data-stu-id="32616-254">Regressors in Decision Tree Models</span></span>  
 <span data-ttu-id="32616-255">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 선형 회귀 알고리즘을 사용하지 않는 경우라도 연속 숫자 입력 및 출력을 포함하는 의사 결정 트리 모델에는 연속 특성에 대한 회귀를 나타내는 노드가 포함될 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-255">Even if you do not use the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Linear Regression algorithm, any decision tree model that has continuous numeric inputs and outputs can potentially include nodes that represent a regression on a continuous attribute.</span></span>  
  
 <span data-ttu-id="32616-256">연속 숫자 데이터 열이 회귀 변수를 나타내도록 지정할 필요는 없습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-256">You do not need to specify that a column of continuous numeric data represents a regressor.</span></span> <span data-ttu-id="32616-257">열에 REGRESSOR 플래그를 설정하지 않았더라도 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 의사 결정 트리 알고리즘은 자동으로 열을 잠재적 회귀 변수로 사용하고 데이터 세트를 의미 있는 패턴이 있는 영역으로 분할합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-257">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm will automatically use the column as a potential regressor and partition the dataset into regions with meaningful patterns even if you do not set the REGRESSOR flag on the column.</span></span>  
  
 <span data-ttu-id="32616-258">그러나 FORCE_REGRESSOR 매개 변수를 사용하면 알고리즘이 항상 특정 회귀 변수를 사용하도록 할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-258">However, you can use the FORCE_REGRESSOR parameter to guarantee that the algorithm will use a particular regressor.</span></span> <span data-ttu-id="32616-259">이 매개 변수는 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 의사 결정 트리 알고리즘과 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 선형 회귀 알고리즘에서만 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-259">This parameter can be used only with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees and [!INCLUDE[msCoName](../../includes/msconame-md.md)] Linear Regression algorithms.</span></span> <span data-ttu-id="32616-260">모델링 플래그를 설정 하면 알고리즘은 a \* C1 + b \* C2 + ... 형식의 회귀 수식을 찾으려고 합니다. 트리의 노드에 패턴을 맞추려면입니다.</span><span class="sxs-lookup"><span data-stu-id="32616-260">When you set the modeling flag, the algorithm will try to find regression equations of the form a\*C1 + b\*C2 + ... to fit the patterns in the nodes of the tree.</span></span> <span data-ttu-id="32616-261">잉여에 대한 합계가 계산되며 편차가 너무 클 경우 트리에서 강제로 분할이 수행됩니다.</span><span class="sxs-lookup"><span data-stu-id="32616-261">The sum of the residuals is calculated, and if the deviation is too great, a split is forced in the tree.</span></span>  
  
 <span data-ttu-id="32616-262">예를 들어 **Income** 을 특성으로 사용하여 고객의 구매 행동을 예측하며 열에 REGRESSOR 모델링 플래그를 설정하는 경우 알고리즘은 먼저 표준 회귀 수식을 사용하여 **Income** 값을 맞추려고 시도합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-262">For example, if you are predicting customer purchasing behavior using **Income** as an attribute, and set the REGRESSOR modeling flag on the column, the algorithm will first try to fit the **Income** values by using a standard regression formula.</span></span> <span data-ttu-id="32616-263">편차가 너무 클 경우 회귀 수식이 중단되고 다른 특성에 따라 트리가 분할됩니다.</span><span class="sxs-lookup"><span data-stu-id="32616-263">If the deviation is too great, the regression formula is abandoned and the tree will be split on another attribute.</span></span> <span data-ttu-id="32616-264">분할 후 의사 결정 트리 알고리즘은 먼저 각 분기에서 Income에 대한 회귀 변수를 맞추려고 시도합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-264">The decision tree algorithm will then try to fit a regressor for income in each of the branches after the split.</span></span>  
  
## <a name="requirements"></a><span data-ttu-id="32616-265">요구 사항</span><span class="sxs-lookup"><span data-stu-id="32616-265">Requirements</span></span>  
 <span data-ttu-id="32616-266">의사 결정 트리 모델은 하나의 키 열, 여러 개의 입력 열 및 하나 이상의 예측 가능한 열을 포함해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-266">A decision tree model must contain a key column, input columns, and at least one predictable column.</span></span>  
  
### <a name="input-and-predictable-columns"></a><span data-ttu-id="32616-267">입력 열과 예측 가능한 열</span><span class="sxs-lookup"><span data-stu-id="32616-267">Input and Predictable Columns</span></span>  
 <span data-ttu-id="32616-268">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 의사 결정 트리 알고리즘은 다음 표에 나열된 특정 입력 열과 예측 가능한 열을 지원합니다.</span><span class="sxs-lookup"><span data-stu-id="32616-268">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm supports the specific input columns and predictable columns that are listed in the following table.</span></span> <span data-ttu-id="32616-269">마이닝 모델에 사용되는 경우 콘텐츠 형식의 의미에 대한 자세한 내용은 [콘텐츠 형식&#40;데이터 마이닝&#41;](content-types-data-mining.md)을 참조하세요.</span><span class="sxs-lookup"><span data-stu-id="32616-269">For more information about what the content types mean when used in a mining model, see [Content Types &#40;Data Mining&#41;](content-types-data-mining.md).</span></span>  
  
|<span data-ttu-id="32616-270">열</span><span class="sxs-lookup"><span data-stu-id="32616-270">Column</span></span>|<span data-ttu-id="32616-271">내용 유형</span><span class="sxs-lookup"><span data-stu-id="32616-271">Content types</span></span>|  
|------------|-------------------|  
|<span data-ttu-id="32616-272">입력 특성</span><span class="sxs-lookup"><span data-stu-id="32616-272">Input attribute</span></span>|<span data-ttu-id="32616-273">Continuous, Cyclical, Discrete, Discretized, Key, Ordered, Table</span><span class="sxs-lookup"><span data-stu-id="32616-273">Continuous, Cyclical, Discrete, Discretized, Key, Ordered, Table</span></span>|  
|<span data-ttu-id="32616-274">예측 가능한 특성</span><span class="sxs-lookup"><span data-stu-id="32616-274">Predictable attribute</span></span>|<span data-ttu-id="32616-275">Continuous, Cyclical, Discrete, Discretized, Ordered, Table</span><span class="sxs-lookup"><span data-stu-id="32616-275">Continuous, Cyclical, Discrete, Discretized, Ordered, Table</span></span>|  
  
> [!NOTE]  
>  <span data-ttu-id="32616-276">Cyclical  및 Ordered  내용 유형이 지원되기는 하지만 알고리즘은 해당 유형을 불연속 값으로 처리하고 특수한 처리를 수행하지 않습니다.</span><span class="sxs-lookup"><span data-stu-id="32616-276">Cyclical and Ordered content types are supported, but the algorithm treats them as discrete values and does not perform special processing.</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="32616-277">참고 항목</span><span class="sxs-lookup"><span data-stu-id="32616-277">See Also</span></span>  
 <span data-ttu-id="32616-278">[Microsoft 의사 결정 트리 알고리즘](microsoft-decision-trees-algorithm.md) </span><span class="sxs-lookup"><span data-stu-id="32616-278">[Microsoft Decision Trees Algorithm](microsoft-decision-trees-algorithm.md) </span></span>  
 <span data-ttu-id="32616-279">[의사 결정 트리 모델 쿼리 예제](decision-trees-model-query-examples.md) </span><span class="sxs-lookup"><span data-stu-id="32616-279">[Decision Trees Model Query Examples](decision-trees-model-query-examples.md) </span></span>  
 [<span data-ttu-id="32616-280">의사 결정 트리 모델에 대한 마이닝 모델 콘텐츠&#40;Analysis Services - 데이터 마이닝&#41;</span><span class="sxs-lookup"><span data-stu-id="32616-280">Mining Model Content for Decision Tree Models &#40;Analysis Services - Data Mining&#41;</span></span>](mining-model-content-for-decision-tree-models-analysis-services-data-mining.md)  
  
  
